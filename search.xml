<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Fixmatch主要代码注释</title>
    <url>/article/2022111294813030.html</url>
    <content><![CDATA[<h1 id="fixmatch">Fixmatch</h1>
<h2 id="设置种子">设置种子</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">set_seed</span>(<span class="params">args</span>):</span><br><span class="line">    random.seed(args.seed) <span class="comment"># python的随机性</span></span><br><span class="line">    np.random.seed(args.seed) <span class="comment"># np的随机性</span></span><br><span class="line">    torch.manual_seed(args.seed) <span class="comment"># torch的CPU随机性，为CPU设置随机种子</span></span><br><span class="line">    <span class="keyword">if</span> args.n_gpu &gt; <span class="number">0</span>:</span><br><span class="line">        torch.cuda.manual_seed_all(args.seed) <span class="comment"># torch的GPU随机性，为所有GPU设置随机种子</span></span><br></pre></td></tr></table></figure>
<ol type="1">
<li>设置随机种子</li>
<li>将种子赋予np</li>
<li>将种子赋予torch</li>
<li>将种子赋予cuda</li>
</ol>
<h2 id="gpu设置">GPU设置</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> args.local_rank == -<span class="number">1</span>:</span><br><span class="line">    device = torch.device(<span class="string">'cuda'</span>, args.gpu_id)</span><br><span class="line">    args.world_size = <span class="number">1</span></span><br><span class="line">    args.n_gpu = torch.cuda.device_count()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    torch.cuda.set_device(args.local_rank)</span><br><span class="line">    device = torch.device(<span class="string">'cuda'</span>, args.local_rank)</span><br><span class="line">    torch.distributed.init_process_group(backend=<span class="string">'nccl'</span>)</span><br><span class="line">    args.world_size = torch.distributed.get_world_size()</span><br><span class="line">    args.n_gpu = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>根据local_rank决定是否采取分布式。如果local_rank=-1，说明分布式失效；如果local_rank不等于-1，则根据不同的卡配置不同的进程数；获取设备device方便后续将数据和模型加载在上面（代码为.to(device))；初始化设置分布式的后端等。</p>
<p><strong>torch.distributed.barrier()的使用：</strong></p>
<p>①数据集：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> args.local_rank <span class="keyword">not</span> <span class="keyword">in</span> [-<span class="number">1</span>, <span class="number">0</span>]:</span><br><span class="line">    torch.distributed.barrier()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取数据集</span></span><br><span class="line">labeled_dataset, unlabeled_dataset, test_dataset = DATASET_GETTERS[args.dataset](</span><br><span class="line">    args, <span class="string">'./data'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.local_rank == <span class="number">0</span>:</span><br><span class="line">    torch.distributed.barrier()</span><br></pre></td></tr></table></figure>
<p>有些操作是不需要多卡同时运行的，如数据集和模型的加载。因此，PyTorch对非主进程的卡上面的运行进行了barrier设置。如果是在并行训练非主卡上，其它进行需要先等待主进程读取并缓存数据集，再从缓存中读取数据，以同步不同进程的数据，避免出现数据处理不同步的现象。</p>
<p>②模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> args.local_rank <span class="keyword">not</span> <span class="keyword">in</span> [-<span class="number">1</span>, <span class="number">0</span>]:</span><br><span class="line">    torch.distributed.barrier()</span><br><span class="line"></span><br><span class="line">model = create_model(args)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.local_rank == <span class="number">0</span>:</span><br><span class="line">    torch.distributed.barrier()</span><br></pre></td></tr></table></figure>
<p>先对其余进程设置一个障碍，等到主进程加载完模型和数据后，再对主进程设置障碍，使所有进程都处于同一“出发线”，最后再同时释放。</p>
<span id="more"></span>
<h2 id="数据集划分">数据集划分</h2>
<p>本代码使用的数据集分为三类：带标签的训练集，不带标签的训练集，测试集。虽然表面上需要一个训练集是“不带标签”的，但是PyTorch并没有直接舍去标签的数据集设置。一开始我在想，如果是我自己来写代码，应该要怎么处理呢？后来发现代码根本没有拘泥于“不带标签”这个事情，因为在返回数据集和标签时，使用“_”直接代替掉标签即可，损失函数也不需要使用标签。</p>
<p>核心API： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">labeled_dataset, unlabeled_dataset, test_dataset = DATASET_GETTERS[args.dataset](args, <span class="string">'./data'</span>)</span><br></pre></td></tr></table></figure></p>
<p>dataset-&gt;cifar.py，发现调用了如下函数（get_cifar10和get_cifar100极其类似，只是数据集分类的类别数不一样而已。下面仅以get_cifar100为例）：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_cifar100(args, root):</span><br><span class="line">    # 图像变换</span><br><span class="line">    transform_labeled = transforms.Compose([</span><br><span class="line">        transforms.RandomHorizontalFlip(),</span><br><span class="line">        transforms.RandomCrop(size=32,</span><br><span class="line">                              padding=int(32*0.125),</span><br><span class="line">                              padding_mode='reflect'),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize(mean=cifar100_mean, std=cifar100_std)])</span><br><span class="line"></span><br><span class="line">    transform_val = transforms.Compose([</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize(mean=cifar100_mean, std=cifar100_std)])</span><br><span class="line">   </span><br><span class="line">    # 数据集设置</span><br><span class="line">    base_dataset = datasets.CIFAR100(</span><br><span class="line">        root, train=True, download=True)</span><br><span class="line"></span><br><span class="line">    train_labeled_idxs, train_unlabeled_idxs = x_u_split(</span><br><span class="line">        args, base_dataset.targets)</span><br><span class="line"></span><br><span class="line">    train_labeled_dataset = CIFAR100SSL(</span><br><span class="line">        root, train_labeled_idxs, train=True,</span><br><span class="line">        transform=transform_labeled)</span><br><span class="line"></span><br><span class="line">    train_unlabeled_dataset = CIFAR100SSL(</span><br><span class="line">        root, train_unlabeled_idxs, train=True,</span><br><span class="line">        transform=TransformFixMatch(mean=cifar100_mean, std=cifar100_std))</span><br><span class="line"></span><br><span class="line">    test_dataset = datasets.CIFAR100(</span><br><span class="line">        root, train=False, transform=transform_val, download=False)</span><br><span class="line"></span><br><span class="line">    return train_labeled_dataset, train_unlabeled_dataset, test_dataset</span><br></pre></td></tr></table></figure>
<p>get_cifar100函数包括两部分：transform的设置和数据集设置。</p>
<p>（1）transform</p>
<p>对于测试集和带标签的训练集，可以根据论文[1]的介绍进行设置。但是对于不带标签的训练集，代码调用了TransformFixMatch类，因为这部分的训练集需要使用弱增强和强增强的方法，两种方法是不同的，所以需要特别设置一个callable的类，能够将两种transform手段凑在一块。当构建dataset调用transform时，可以直接调用call函数，直接返回两个增强手段处理后的图像。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformFixMatch</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, mean, std</span>):</span><br><span class="line">        self.weak = transforms.Compose([</span><br><span class="line">            transforms.RandomHorizontalFlip(),</span><br><span class="line">            transforms.RandomCrop(size=<span class="number">32</span>,</span><br><span class="line">                                  padding=<span class="built_in">int</span>(<span class="number">32</span>*<span class="number">0.125</span>),</span><br><span class="line">                                  padding_mode=<span class="string">'reflect'</span>)])</span><br><span class="line">        self.strong = transforms.Compose([</span><br><span class="line">            transforms.RandomHorizontalFlip(),</span><br><span class="line">            transforms.RandomCrop(size=<span class="number">32</span>,</span><br><span class="line">                                  padding=<span class="built_in">int</span>(<span class="number">32</span>*<span class="number">0.125</span>),</span><br><span class="line">                                  padding_mode=<span class="string">'reflect'</span>),</span><br><span class="line">            RandAugmentMC(n=<span class="number">2</span>, m=<span class="number">10</span>)])</span><br><span class="line">        self.normalize = transforms.Compose([</span><br><span class="line">            transforms.ToTensor(),</span><br><span class="line">            transforms.Normalize(mean=mean, std=std)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, x</span>):</span><br><span class="line">        weak = self.weak(x)</span><br><span class="line">        strong = self.strong(x)</span><br><span class="line">        <span class="keyword">return</span> self.normalize(weak), self.normalize(strong)</span><br></pre></td></tr></table></figure>
<p>（2）数据索引设置</p>
<p>怎么从原始的CIFAR数据集提取出带标签的训练集和无标签的训练集？注意到PyTorch数据集类有一个函数成员def
<strong>getitem</strong>(self,
index)，核心参数是index，所以我们构建以上两个训练集，本质上是构建训练集对应的索引值。下面是索引生成函数x_u_split的代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def x_u_split(args, labels):</span><br><span class="line">    label_per_class = args.num_labeled // args.num_classes</span><br><span class="line">    labels = np.array(labels) #每个label是一个数字</span><br><span class="line">    labeled_idx = []</span><br><span class="line">    unlabeled_idx = np.array(range(len(labels)))</span><br><span class="line">    for i in range(args.num_classes): </span><br><span class="line">        idx = np.where(labels == i)[0] #有[0]是因为np.where得到的是一个tuple,需要把tuple的元素提取出来</span><br><span class="line">        idx = np.random.choice(idx, label_per_class, False) </span><br><span class="line">        labeled_idx.extend(idx)</span><br><span class="line">    labeled_idx = np.array(labeled_idx)</span><br><span class="line">    assert len(labeled_idx) == args.num_labeled</span><br><span class="line"></span><br><span class="line">    if args.expand_labels or args.num_labeled &lt; args.batch_size: </span><br><span class="line">        num_expand_x = math.ceil( #向上取整</span><br><span class="line">            args.batch_size * args.eval_step / args.num_labeled) #等于17</span><br><span class="line"></span><br><span class="line">        #将参数元组的元素数组按水平方向进行叠加</span><br><span class="line">        labeled_idx = np.hstack([labeled_idx for _ in range(num_expand_x)])</span><br><span class="line">    np.random.shuffle(labeled_idx)</span><br><span class="line">    return labeled_idx, unlabeled_idx</span><br></pre></td></tr></table></figure>
<p>每个类带标签数据的个数是均衡的，每个类带标签的数据个数 =
带标签数据总个数//类数，所以，使用一个循环（10个类）。
对于每一个类，找出他们在总数据（labels）中的数据索引，然后将labels（原本是列表）转换为numpy数组。并用random.choice随机选择label_per_class个数据，将他们加入到带标签的数据索引labeled_idx中。
对于不带标签的数据，原文作者使用了所有的数据（包含带标签的数据），所以他的索引为全部数据的索引，unlabeled_idx可以直接对应全体数据。
需要注意的一个点是，args.expand_labels参数作者默认为true的，所以我们要进行数据重复。
或者num_labeled比batch_size还小，则对数组进行扩充。
这里重复的次数num_expand_x为 64（batch_size ）* 1024（eval_step）/ 4000
（num_labeled）=17次 所以带标签的数据为
68000个（每个索引都重复了17次）。</p>
<p>（3）数据集设置</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CIFAR100SSL</span>(datasets.CIFAR100):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root, indexs, train=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                 transform=<span class="literal">None</span>, target_transform=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 download=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(root, train=train,</span><br><span class="line">                         transform=transform,</span><br><span class="line">                         target_transform=target_transform,</span><br><span class="line">                         download=download)</span><br><span class="line">        <span class="keyword">if</span> indexs <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.data = self.data[indexs]</span><br><span class="line">            self.targets = np.array(self.targets)[indexs]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        img, target = self.data[index], self.targets[index]</span><br><span class="line">        img = Image.fromarray(img)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            img = self.transform(img)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.target_transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            target = self.target_transform(target)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> img, target</span><br></pre></td></tr></table></figure>
<h2 id="scheduler">scheduler</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_cosine_schedule_with_warmup</span>(<span class="params">optimizer,</span></span><br><span class="line"><span class="params">                                    num_warmup_steps,</span></span><br><span class="line"><span class="params">                                    num_training_steps,</span></span><br><span class="line"><span class="params">                                    num_cycles=<span class="number">7.</span>/<span class="number">16.</span>,</span></span><br><span class="line"><span class="params">                                    last_epoch=-<span class="number">1</span></span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_lr_lambda</span>(<span class="params">current_step</span>):</span><br><span class="line">        <span class="keyword">if</span> current_step &lt; num_warmup_steps:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">float</span>(current_step) / <span class="built_in">float</span>(<span class="built_in">max</span>(<span class="number">1</span>, num_warmup_steps))</span><br><span class="line">        no_progress = <span class="built_in">float</span>(current_step - num_warmup_steps) / \</span><br><span class="line">            <span class="built_in">float</span>(<span class="built_in">max</span>(<span class="number">1</span>, num_training_steps - num_warmup_steps))</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">max</span>(<span class="number">0.</span>, math.cos(math.pi * num_cycles * no_progress))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> LambdaLR(optimizer, _lr_lambda, last_epoch)</span><br><span class="line"><span class="comment"># LambdaLR设置学习率为初始学习率乘以给定lr_lambda函数的值</span></span><br><span class="line"><span class="comment"># 当last_epoch=-1时, base_lr为optimizer优化器中的lr</span></span><br><span class="line"><span class="comment"># 每次执行 scheduler.step(),  last_epoch=last_epoch +1</span></span><br></pre></td></tr></table></figure>
<p>scheduler是为了动态调整训练期间的学习率，使模型更好地收敛。论文使用的是带有warmup性质的余弦退火学习率调整器。核心是返回了一个自定义函数的学习率调整器，调整的函数是_lr_lambda，如果当前的step少于warmup的步数，则使用线性递增的策略一直增加到初始学习率；而后使用余弦变化的策略改变学习率:
<span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -1.602ex;" xmlns="http://www.w3.org/2000/svg" width="15.563ex" height="4.701ex" role="img" focusable="false" viewBox="0 -1370 6878.8 2078"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D702" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q156 442 175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336V326Q503 302 439 53Q381 -182 377 -189Q364 -216 332 -216Q319 -216 310 -208T299 -186Q299 -177 358 57L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mn" transform="translate(530,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(1211.3,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(2267.1,0)"><path data-c="63" d="M370 305T349 305T313 320T297 358Q297 381 312 396Q317 401 317 402T307 404Q281 408 258 408Q209 408 178 376Q131 329 131 219Q131 137 162 90Q203 29 272 29Q313 29 338 55T374 117Q376 125 379 127T395 129H409Q415 123 415 120Q415 116 411 104T395 71T366 33T318 2T249 -11Q163 -11 99 53T34 214Q34 318 99 383T250 448T370 421T404 357Q404 334 387 320Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(444,0)"></path><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" transform="translate(944,0)"></path></g><g data-mml-node="mo" transform="translate(3605.1,0)"><path data-c="2061" d=""></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(3771.8,0)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mfrac" transform="translate(389,0)"><g data-mml-node="mrow" transform="translate(369,676)"><g data-mml-node="mn"><path data-c="37" d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z"></path></g><g data-mml-node="mi" transform="translate(500,0)"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="mi" transform="translate(1070,0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g><g data-mml-node="mrow" transform="translate(220,-686)"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z" transform="translate(500,0)"></path></g><g data-mml-node="mi" transform="translate(1000,0)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g><rect width="2089" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(2718,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></g></svg></mjx-container></span> <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.489ex;" xmlns="http://www.w3.org/2000/svg" width="1.124ex" height="1.489ex" role="img" focusable="false" viewBox="0 -442 497 658"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D702" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q156 442 175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336V326Q503 302 439 53Q381 -182 377 -189Q364 -216 332 -216Q319 -216 310 -208T299 -186Q299 -177 358 57L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container></span>是初始学习率，<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.179ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 521 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g></svg></mjx-container></span>是当前的步数，<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.011ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 889 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g></g></svg></mjx-container></span>是总步数。</p>
<h2 id="混合精度">混合精度</h2>
<p>本代码使用的是英伟达开发的apex库，可以通过使用混合精度，在保证精度丢失很少的情况下，减少内存，增快训练速度。混合精度涉及对模型和优化器的重初始化、损失函数的反向传播等。代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> apex <span class="keyword">import</span> amp</span><br><span class="line">model, optimizer = amp.initialize(model, optimizer, opt_level=args.opt_level)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> amp.scale_loss(loss, optimizer) <span class="keyword">as</span> scaled_loss:</span><br><span class="line">    scaled_loss.backward()</span><br></pre></td></tr></table></figure>
<h2 id="指数移动平均ema">指数移动平均（EMA）</h2>
<p>EMA在本代码是用于更新模型权重的，核心公式就这一条： <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="21.597ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 9545.9 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mi" transform="translate(518,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(1101,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(2156.8,0)"><path data-c="1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path></g><g data-mml-node="msub" transform="translate(2722.8,0)"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="TeXAtom" transform="translate(518,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(361,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1139,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(4672,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mo" transform="translate(5672.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(6061.2,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(6783.4,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(7783.7,0)"><path data-c="1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path></g><g data-mml-node="mo" transform="translate(8349.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="msub" transform="translate(8738.7,0)"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mi" transform="translate(502,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g></g></svg></mjx-container></span> 这里的参数<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.097ex" height="1.027ex" role="img" focusable="false" viewBox="0 -443 485 454"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g></g></g></svg></mjx-container></span>代表测试用模型的参数权重。训练时，原模型就按照正常的节奏来训练、更新权重，而另外开辟一个EMA模型，在原模型更新权重的同时也跟着更新权重，并作为最后使用的模型，检测在测试集上的表现。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ModelEMA</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, args, model, decay</span>):</span><br><span class="line">        self.ema = deepcopy(model)</span><br><span class="line">        self.ema.to(args.device)</span><br><span class="line">        self.ema.<span class="built_in">eval</span>()</span><br><span class="line">        self.decay = decay</span><br><span class="line">        self.ema_has_module = <span class="built_in">hasattr</span>(self.ema, <span class="string">'module'</span>)</span><br><span class="line">        <span class="comment"># Fix EMA. https://github.com/valencebond/FixMatch_pytorch thank you!</span></span><br><span class="line">        self.param_keys = [k <span class="keyword">for</span> k, _ <span class="keyword">in</span> self.ema.named_parameters()]</span><br><span class="line">        self.buffer_keys = [k <span class="keyword">for</span> k, _ <span class="keyword">in</span> self.ema.named_buffers()]</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.ema.parameters():</span><br><span class="line">            p.requires_grad_(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, model</span>): <span class="comment"># 核心模块</span></span><br><span class="line">        <span class="comment"># hasattr函数用于判断对象是否包含对应的属性。</span></span><br><span class="line">        needs_module = <span class="built_in">hasattr</span>(model, <span class="string">'module'</span>) <span class="keyword">and</span> <span class="keyword">not</span> self.ema_has_module</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            msd = model.state_dict() <span class="comment"># torch.nn.Module模块中的state_dict变量存放训练过程中需要学习的权重</span></span><br><span class="line">            esd = self.ema.state_dict()</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> self.param_keys:</span><br><span class="line">                <span class="keyword">if</span> needs_module:</span><br><span class="line">                    j = <span class="string">'module.'</span> + k</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    j = k</span><br><span class="line">                model_v = msd[j].detach()</span><br><span class="line">                ema_v = esd[k]</span><br><span class="line">                <span class="comment"># ema_v是过去的平均状态，model_v是现在的参数</span></span><br><span class="line">                esd[k].copy_(ema_v * self.decay + (<span class="number">1.</span> - self.decay) * model_v)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> self.buffer_keys:</span><br><span class="line">                <span class="keyword">if</span> needs_module:</span><br><span class="line">                    j = <span class="string">'module.'</span> + k</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    j = k</span><br><span class="line">                esd[k].copy_(msd[j])</span><br></pre></td></tr></table></figure>
<h2 id="权重衰减weight-decay">权重衰减（Weight Decay）</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">grouped_parameters = [</span><br><span class="line">        <span class="comment"># 若网络层不包含 bias 或 BatchNorm，则应用 weight_decay</span></span><br><span class="line">        <span class="comment"># any() 函数用于判断给定的可迭代参数 iterable 是否全部为 False，则返回 False，如果有一个为 True，则返回 True</span></span><br><span class="line">        {<span class="string">'params'</span>: [p <span class="keyword">for</span> n, p <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">any</span>(</span><br><span class="line">            nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay)], <span class="string">'weight_decay'</span>: args.wdecay},</span><br><span class="line">        <span class="comment"># 反之，则不用 weight_decay</span></span><br><span class="line">        {<span class="string">'params'</span>: [p <span class="keyword">for</span> n, p <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> <span class="built_in">any</span>(</span><br><span class="line">            nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay)], <span class="string">'weight_decay'</span>: <span class="number">0.0</span>}</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure>
<h2 id="核心算法">核心算法</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">labeled_iter = <span class="built_in">iter</span>(labeled_trainloader)</span><br><span class="line">unlabeled_iter = <span class="built_in">iter</span>(unlabeled_trainloader)</span><br><span class="line"></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.start_epoch, args.epochs):</span><br><span class="line">    <span class="comment"># 平均处理器，用于存储一些统计信息</span></span><br><span class="line">    batch_time = AverageMeter()</span><br><span class="line">    data_time = AverageMeter()</span><br><span class="line">    losses = AverageMeter()</span><br><span class="line">    losses_x = AverageMeter()</span><br><span class="line">    losses_u = AverageMeter()</span><br><span class="line">    mask_probs = AverageMeter()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> args.no_progress:</span><br><span class="line">        p_bar = tqdm(<span class="built_in">range</span>(args.eval_step),</span><br><span class="line">                     disable=args.local_rank <span class="keyword">not</span> <span class="keyword">in</span> [-<span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> batch_idx <span class="keyword">in</span> <span class="built_in">range</span>(args.eval_step):</span><br><span class="line">        <span class="comment"># 使用iter(next)读取指定次数的batch，而不通过Dataloader。Dataloader的长度也不同</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            inputs_x, targets_x = labeled_iter.<span class="built_in">next</span>()</span><br><span class="line">            <span class="comment">#print(inputs_x.shape) # torch.Size([64, 3, 32, 32])</span></span><br><span class="line">            <span class="comment">#print(targets_x.shape) # torch.Size([64])</span></span><br><span class="line">            <span class="comment">#print(targets_x)</span></span><br><span class="line">        <span class="keyword">except</span>: <span class="comment"># 当循环结束时，重新开始循环</span></span><br><span class="line">            <span class="keyword">if</span> args.world_size &gt; <span class="number">1</span>:</span><br><span class="line">                labeled_epoch += <span class="number">1</span></span><br><span class="line">                labeled_trainloader.sampler.set_epoch(labeled_epoch)</span><br><span class="line">            labeled_iter = <span class="built_in">iter</span>(labeled_trainloader)</span><br><span class="line">            inputs_x, targets_x = labeled_iter.<span class="built_in">next</span>()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            (inputs_u_w, inputs_u_s), _ = unlabeled_iter.<span class="built_in">next</span>()</span><br><span class="line">            <span class="comment">#print(inputs_u_w.shape) #torch.Size([448, 3, 32, 32])</span></span><br><span class="line">            <span class="comment">#print(inputs_u_s.shape) #torch.Size([448, 3, 32, 32])</span></span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">if</span> args.world_size &gt; <span class="number">1</span>:</span><br><span class="line">                unlabeled_epoch += <span class="number">1</span></span><br><span class="line">                unlabeled_trainloader.sampler.set_epoch(unlabeled_epoch)</span><br><span class="line">            unlabeled_iter = <span class="built_in">iter</span>(unlabeled_trainloader)</span><br><span class="line">            (inputs_u_w, inputs_u_s), _ = unlabeled_iter.<span class="built_in">next</span>() <span class="comment"># 忽略标签</span></span><br><span class="line"></span><br><span class="line">        data_time.update(time.time() - end)</span><br><span class="line">        batch_size = inputs_x.shape[<span class="number">0</span>] <span class="comment"># 64</span></span><br><span class="line">        <span class="comment"># 带标签的数据每批次有B个，无标签数据每批次有μB个(每个2张图像)，加起来就是(2μ+1)B个</span></span><br><span class="line">        inputs = interleave(</span><br><span class="line">            torch.cat((inputs_x, inputs_u_w, inputs_u_s)), <span class="number">2</span>*args.mu+<span class="number">1</span>).to(args.device)</span><br><span class="line">        <span class="comment"># print(inputs.shape) #torch.Size([960, 3, 32, 32]) 960=448+448+64 960=64*(2*7+1) 将数据合并一起</span></span><br><span class="line">        targets_x = targets_x.to(args.device)</span><br><span class="line">        <span class="comment"># print(targets_x.shape) #torch.Size([64])</span></span><br><span class="line">        </span><br><span class="line">        logits = model(inputs)</span><br><span class="line">        <span class="comment">#print(logits.shape) #torch.Size([960, 10])</span></span><br><span class="line">        logits = de_interleave(logits, <span class="number">2</span>*args.mu+<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#print(logits.shape) #torch.Size([960, 10])</span></span><br><span class="line">        logits_x = logits[:batch_size] <span class="comment"># 前B个</span></span><br><span class="line">        <span class="comment">#print(logits_x.shape) #torch.Size([64, 10])</span></span><br><span class="line">        logits_u_w, logits_u_s = logits[batch_size:].chunk(<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># torch.chunk 将输入Tensor拆分为特定数量的块。如果给定维度dim上的Tensor大小不能够被整除，则最后一个块会小于之前的块。</span></span><br><span class="line">        <span class="comment">#print(logits_u_w.shape) #torch.Size([448, 10]) </span></span><br><span class="line">        <span class="keyword">del</span> logits <span class="comment"># 省出内存，del删除的是变量，而不是数据。</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 带标签数据的损失函数</span></span><br><span class="line">        Lx = F.cross_entropy(logits_x, targets_x, reduction=<span class="string">'mean'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过weak_augment样本计算伪标记pseudo label和mask，</span></span><br><span class="line">        <span class="comment"># 其中，mask用来筛选哪些样本最大预测概率超过阈值，可以拿来使用，哪些不能使用</span></span><br><span class="line"></span><br><span class="line">        pseudo_label = torch.softmax(logits_u_w.detach()/args.T, dim=-<span class="number">1</span>) <span class="comment"># 与 dim=2 等价，对某一维度的行进行softmax运算，和为1</span></span><br><span class="line">        <span class="comment"># Softmax为T＝1时的特例</span></span><br><span class="line">        max_probs, targets_u = torch.<span class="built_in">max</span>(pseudo_label, dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#print(max_probs.shape) # torch.Size([448]) 448个最大概率值</span></span><br><span class="line">        <span class="comment">#print(targets_u.shape) # torch.Size([448]) 448个伪标签的值，实际上是pseudo_label中最大位置的索引</span></span><br><span class="line">        <span class="comment">#print(targets_u) #tensor([3, 5, 1 ....], device='cuda:0')</span></span><br><span class="line">        mask = max_probs.ge(args.threshold).<span class="built_in">float</span>() <span class="comment"># greater and equal（大于等于）</span></span><br><span class="line">        <span class="comment"># 比0.95大才说明这个标签置信度高，如果低于这个阈值，即使计算了交叉熵，也会被mask为0</span></span><br><span class="line">        <span class="comment"># torch.ge(a,b)逐个元素比较a，b的大小</span></span><br><span class="line">        <span class="comment"># print(mask.shape) #torch.Size([448]) 448个0/1</span></span><br><span class="line">        <span class="comment"># print(F.cross_entropy(logits_u_s, targets_u,reduction='none')) # reduction='none'不求平均，返回448个值</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 不带标签数据的损失函数</span></span><br><span class="line">        Lu = (F.cross_entropy(logits_u_s, targets_u,</span><br><span class="line">                              reduction=<span class="string">'none'</span>) * mask).mean()</span><br><span class="line"></span><br><span class="line">        loss = Lx + args.lambda_u * Lu <span class="comment"># 完整的损失函数</span></span><br></pre></td></tr></table></figure>
<p>其中torch.max的用法参考如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.randn(<span class="number">24</span>).reshape(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br></pre></td></tr></table></figure>
<p>运行结果:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[[-<span class="number">0.9135</span>,  <span class="number">1.3096</span>,  <span class="number">0.2803</span>, -<span class="number">0.9314</span>],</span><br><span class="line">         [-<span class="number">0.2687</span>, -<span class="number">0.0968</span>, -<span class="number">0.7156</span>, -<span class="number">0.8814</span>],</span><br><span class="line">         [-<span class="number">1.0099</span>,  <span class="number">1.6910</span>,  <span class="number">0.3458</span>, -<span class="number">0.6547</span>]],</span><br><span class="line"></span><br><span class="line">        [[-<span class="number">0.4334</span>, -<span class="number">0.0464</span>, -<span class="number">1.9236</span>,  <span class="number">0.3148</span>],</span><br><span class="line">         [ <span class="number">0.3628</span>, -<span class="number">0.7063</span>, -<span class="number">0.1750</span>,  <span class="number">1.5068</span>],</span><br><span class="line">         [ <span class="number">1.1270</span>, -<span class="number">0.9374</span>, -<span class="number">0.8419</span>, -<span class="number">0.0050</span>]]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = torch.softmax(a.detach()/<span class="number">1</span>, dim=-<span class="number">1</span>) <span class="comment"># 与 dim=2 等价，对某一维度的行进行softmax运算，和为1</span></span><br><span class="line"><span class="comment"># A = torch.softmax(a, dim=-1)</span></span><br><span class="line"><span class="built_in">print</span>(A)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[[<span class="number">0.0689</span>, <span class="number">0.6362</span>, <span class="number">0.2273</span>, <span class="number">0.0677</span>],</span><br><span class="line">         [<span class="number">0.2968</span>, <span class="number">0.3525</span>, <span class="number">0.1899</span>, <span class="number">0.1608</span>],</span><br><span class="line">         [<span class="number">0.0472</span>, <span class="number">0.7025</span>, <span class="number">0.1830</span>, <span class="number">0.0673</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.2079</span>, <span class="number">0.3061</span>, <span class="number">0.0468</span>, <span class="number">0.4392</span>],</span><br><span class="line">         [<span class="number">0.1974</span>, <span class="number">0.0678</span>, <span class="number">0.1153</span>, <span class="number">0.6196</span>],</span><br><span class="line">         [<span class="number">0.6294</span>, <span class="number">0.0799</span>, <span class="number">0.0879</span>, <span class="number">0.2029</span>]]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">max_probs, targets_u = torch.<span class="built_in">max</span>(A, dim=-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(max_probs)</span><br><span class="line"><span class="built_in">print</span>(max_probs.shape)</span><br><span class="line"><span class="built_in">print</span>(targets_u)</span><br><span class="line"><span class="built_in">print</span>(targets_u.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[<span class="number">0.6362</span>, <span class="number">0.3525</span>, <span class="number">0.7025</span>],</span><br><span class="line">        [<span class="number">0.4392</span>, <span class="number">0.6196</span>, <span class="number">0.6294</span>]])</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">3</span>, <span class="number">0</span>]])</span><br><span class="line"><span class="comment"># cifar10标签序号从0-9，一共十类，IN数据集标签序号从1-16，一共十六类</span></span><br><span class="line"><span class="comment"># 而torch.max返回的最大位置索引是从0开始，这会导致序号对不上</span></span><br><span class="line"><span class="comment"># 上句话不对，加载数据集的时候已经把0标签排除了，所以不影响</span></span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<h2 id="模型保存与加载">模型保存与加载</h2>
<h3 id="模型保存过程">模型保存过程</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> args.local_rank <span class="keyword">in</span> [-<span class="number">1</span>, <span class="number">0</span>]:</span><br><span class="line">    test_loss, test_acc = test(args, test_loader, test_model, epoch)</span><br><span class="line">    </span><br><span class="line">    args.writer.add_scalar(<span class="string">'train/1.train_loss'</span>, losses.avg, epoch)</span><br><span class="line">    args.writer.add_scalar(<span class="string">'train/2.train_loss_x'</span>, losses_x.avg, epoch)</span><br><span class="line">    args.writer.add_scalar(<span class="string">'train/3.train_loss_u'</span>, losses_u.avg, epoch)</span><br><span class="line">    args.writer.add_scalar(<span class="string">'train/4.mask'</span>, mask_probs.avg, epoch)</span><br><span class="line">    args.writer.add_scalar(<span class="string">'test/1.test_acc'</span>, test_acc, epoch)</span><br><span class="line">    args.writer.add_scalar(<span class="string">'test/2.test_loss'</span>, test_loss, epoch)</span><br><span class="line">    </span><br><span class="line">    is_best = test_acc &gt; best_acc</span><br><span class="line">    best_acc = <span class="built_in">max</span>(test_acc, best_acc)</span><br><span class="line">    </span><br><span class="line">    model_to_save = model.module <span class="keyword">if</span> <span class="built_in">hasattr</span>(model, <span class="string">"module"</span>) <span class="keyword">else</span> model <span class="comment"># hasattr() 函数用于判断对象是否包含对应的属性</span></span><br><span class="line">    <span class="keyword">if</span> args.use_ema:</span><br><span class="line">        ema_to_save = ema_model.ema.module <span class="keyword">if</span> <span class="built_in">hasattr</span>(</span><br><span class="line">            ema_model.ema, <span class="string">"module"</span>) <span class="keyword">else</span> ema_model.ema</span><br><span class="line">        save_checkpoint({</span><br><span class="line">            <span class="string">'epoch'</span>: epoch + <span class="number">1</span>,</span><br><span class="line">            <span class="string">'state_dict'</span>: model_to_save.state_dict(),</span><br><span class="line">            <span class="string">'ema_state_dict'</span>: ema_to_save.state_dict() <span class="keyword">if</span> args.use_ema <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">            <span class="string">'acc'</span>: test_acc,</span><br><span class="line">            <span class="string">'best_acc'</span>: best_acc,</span><br><span class="line">            <span class="string">'optimizer'</span>: optimizer.state_dict(),</span><br><span class="line">            <span class="string">'scheduler'</span>: scheduler.state_dict(),</span><br><span class="line">        }, is_best, args.out)</span><br><span class="line">        </span><br><span class="line">        test_accs.append(test_acc)</span><br><span class="line">        logger.info(<span class="string">'Best top-1 acc: {:.3f}'</span>.<span class="built_in">format</span>(best_acc))</span><br><span class="line">        logger.info(<span class="string">'Mean top-1 acc: {:.3f}\n'</span>.<span class="built_in">format</span>(</span><br><span class="line">            np.mean(test_accs[-<span class="number">20</span>:])))</span><br></pre></td></tr></table></figure>
<h3 id="状态字典state_dict">状态字典：state_dict：</h3>
<p>在PyTorch中，<code>torch.nn.Module</code>模型的可学习参数（即权重和偏差）包含在模型的参数中，（使用<code>model.parameters()</code>可以进行访问）。
<code>state_dict</code>是Python字典对象，它将每一层映射到其参数张量。注意，只有具有可学习参数的层（如卷积层，线性层等）的模型才具有<code>state_dict</code>这一项。目标优化<code>torch.optim</code>也有<code>state_dict</code>属性，它包含有关优化器的状态信息，以及使用的超参数。
因为state_dict的对象是Python字典，所以它们可以很容易的保存、更新、修改和恢复，为PyTorch模型和优化器添加了大量模块。
下面通过从简单模型训练一个分类器中来了解一下<code>state_dict</code>的使用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TheModelClass</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(TheModelClass, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化模型</span></span><br><span class="line">model = TheModelClass()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化优化器</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印模型的状态字典</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Model's state_dict:"</span>)</span><br><span class="line"><span class="keyword">for</span> param_tensor <span class="keyword">in</span> model.state_dict():</span><br><span class="line">    <span class="built_in">print</span>(param_tensor, <span class="string">"\t"</span>, model.state_dict()[param_tensor].size())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印优化器的状态字典</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Optimizer's state_dict:"</span>)</span><br><span class="line"><span class="keyword">for</span> var_name <span class="keyword">in</span> optimizer.state_dict():</span><br><span class="line">    <span class="built_in">print</span>(var_name, <span class="string">"\t"</span>, optimizer.state_dict()[var_name])</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Model<span class="string">'s state_dict:</span></span><br><span class="line"><span class="string">conv1.weight 	 torch.Size([6, 3, 5, 5])</span></span><br><span class="line"><span class="string">conv1.bias 	 torch.Size([6])</span></span><br><span class="line"><span class="string">conv2.weight 	 torch.Size([16, 6, 5, 5])</span></span><br><span class="line"><span class="string">conv2.bias 	 torch.Size([16])</span></span><br><span class="line"><span class="string">fc1.weight 	 torch.Size([120, 400])</span></span><br><span class="line"><span class="string">fc1.bias 	 torch.Size([120])</span></span><br><span class="line"><span class="string">fc2.weight 	 torch.Size([84, 120])</span></span><br><span class="line"><span class="string">fc2.bias 	 torch.Size([84])</span></span><br><span class="line"><span class="string">fc3.weight 	 torch.Size([10, 84])</span></span><br><span class="line"><span class="string">fc3.bias 	 torch.Size([10])</span></span><br><span class="line"><span class="string">Optimizer'</span>s state_dict:</span><br><span class="line">state 	 {}</span><br><span class="line">param_groups 	 [{<span class="string">'lr'</span>: <span class="number">0.001</span>, <span class="string">'momentum'</span>: <span class="number">0.9</span>, <span class="string">'dampening'</span>: <span class="number">0</span>, <span class="string">'weight_decay'</span>: <span class="number">0</span>, <span class="string">'nesterov'</span>: <span class="literal">False</span>, <span class="string">'maximize'</span>: <span class="literal">False</span>, <span class="string">'params'</span>: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]}]</span><br></pre></td></tr></table></figure>
<h3 id="定义save_checkpoint保存完整模型">定义save_checkpoint保存完整模型</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">save_checkpoint</span>(<span class="params">state, is_best, checkpoint, filename=<span class="string">'checkpoint.pth.tar'</span></span>):</span><br><span class="line">    filepath = os.path.join(checkpoint, filename)</span><br><span class="line">    torch.save(state, filepath) <span class="comment"># 保存模型</span></span><br><span class="line">    <span class="keyword">if</span> is_best:</span><br><span class="line">        shutil.copyfile(filepath, os.path.join(checkpoint, <span class="string">'model_best.pth.tar'</span>)) <span class="comment"># 复制文件</span></span><br></pre></td></tr></table></figure>
<p>当保存好模型用来推断的时候，只需要保存模型学习到的参数，使用<code>torch.save()</code>函数来保存模型<code>state_dict</code>。
在 PyTorch 中最常见的模型保存使‘.pt’或者是‘.pth’作为模型文件扩展名。
在运行推理之前，务必调用<code>model.eval()</code>去设置 dropout 和 batch
normalization 层为评估模式。如果不这么做，可能导致模型推断结果不一致。
注意：
<code>load_state_dict()</code>函数只接受字典对象，而不是保存对象的路径。这就意味着在你传给<code>load_state_dict()</code>函数之前，你必须反序列化你保存的<code>state_dict</code>。例如，你无法通过
<code>model.load_state_dict(PATH)</code>来加载模型。</p>
<h3 id="保存和加载-checkpoint-用于推理继续训练">保存和加载 Checkpoint
用于推理/继续训练</h3>
<h4 id="保存checkpoint"><strong>保存Checkpoint：</strong></h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">save_checkpoint({</span><br><span class="line">            <span class="string">'epoch'</span>: epoch + <span class="number">1</span>,</span><br><span class="line">            <span class="string">'state_dict'</span>: model_to_save.state_dict(),</span><br><span class="line">            <span class="string">'ema_state_dict'</span>: ema_to_save.state_dict() <span class="keyword">if</span> args.use_ema <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">            <span class="string">'acc'</span>: test_acc,</span><br><span class="line">            <span class="string">'best_acc'</span>: best_acc,</span><br><span class="line">            <span class="string">'optimizer'</span>: optimizer.state_dict(),</span><br><span class="line">            <span class="string">'scheduler'</span>: scheduler.state_dict(),</span><br><span class="line">        }, is_best, args.out)</span><br></pre></td></tr></table></figure>
<p>当保存成 Checkpoint
的时候，可用于推理或者是继续训练，保存的不仅仅是模型的<code>state_dict</code>。保存优化器的<code>state_dict</code>也很重要,
因为它包含作为模型训练更新的缓冲区和参数。你也许想保存其他项目，比如最新记录的训练损失，外部的<code>torch.nn.Embedding</code>层等等。
要保存多个组件，请在字典中组织它们并使用<code>torch.save()</code>来序列化字典。PyTorch
中常见的保存checkpoint是使用 .tar 文件扩展名。
要加载项目，首先需要初始化模型和优化器，然后使用<code>torch.load()</code>来加载本地字典。这里，你可以非常容易的通过简单查询字典来访问你所保存的项目。
请记住在运行推理之前，务必调用<code>model.eval()</code>去设置 dropout 和
batch normalization 为评估。如果不这样做，有可能得到不一致的推断结果。
如果你想要恢复训练，请调用<code>model.train()</code>以确保这些层处于训练模式。</p>
<h4 id="加载checkpoint"><strong>加载Checkpoint：</strong></h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> args.resume:</span><br><span class="line">    logger.info(<span class="string">"==&gt; Resuming from checkpoint.."</span>)</span><br><span class="line">    <span class="comment"># os.path.isfile判断某一对象(需提供绝对路径)是否为文件</span></span><br><span class="line">    <span class="keyword">assert</span> os.path.isfile(</span><br><span class="line">        args.resume), <span class="string">"Error: no checkpoint directory found!"</span></span><br><span class="line">    args.out = os.path.dirname(args.resume)</span><br><span class="line">    checkpoint = torch.load(args.resume)</span><br><span class="line">    best_acc = checkpoint[<span class="string">'best_acc'</span>]</span><br><span class="line">    args.start_epoch = checkpoint[<span class="string">'epoch'</span>]</span><br><span class="line">    model.load_state_dict(checkpoint[<span class="string">'state_dict'</span>])</span><br><span class="line">    <span class="keyword">if</span> args.use_ema:</span><br><span class="line">        ema_model.ema.load_state_dict(checkpoint[<span class="string">'ema_state_dict'</span>])</span><br><span class="line">    optimizer.load_state_dict(checkpoint[<span class="string">'optimizer'</span>])</span><br><span class="line">    scheduler.load_state_dict(checkpoint[<span class="string">'scheduler'</span>])</span><br></pre></td></tr></table></figure>
<h4 id="加载最优模型">加载最优模型：</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 自己写的</span></span><br><span class="line">filepath = os.path.join(args.out, <span class="string">'model_best.pth.tar'</span>)</span><br><span class="line">    <span class="keyword">assert</span> os.path.isfile(filepath), <span class="string">"Error: no model_best directory found!"</span></span><br><span class="line">    model.load_state_dict(torch.load(filepath)[<span class="string">'state_dict'</span>])</span><br><span class="line">    <span class="keyword">if</span> args.use_ema:</span><br><span class="line">        ema_model.ema.load_state_dict(torch.load(filepath)[<span class="string">'ema_state_dict'</span>])</span><br></pre></td></tr></table></figure>
<h2 id="accuracy">accuracy</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># outputs.shape为 [batch_size, category_count]</span></span><br><span class="line"><span class="comment"># targets.shape为 [batch_size]，每个样本中只有一个真实的类</span></span><br><span class="line"><span class="comment"># topk是要包含在精度中的类的元组，例如topk=(1,5)，则包含五个类</span></span><br><span class="line"><span class="comment"># topk必须是一个元组，所以给出一个数字，不要忘记逗号</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">output, target, topk=(<span class="params"><span class="number">1</span>,</span>)</span>):</span><br><span class="line">    <span class="string">"""Computes the precision@k for the specified values of k"""</span></span><br><span class="line">    maxk = <span class="built_in">max</span>(topk)</span><br><span class="line">    <span class="comment"># size函数：总元素的个数</span></span><br><span class="line">    batch_size = target.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># topk函数output的d维度中选取前k个最大的值，下式为前maxk个</span></span><br><span class="line">    <span class="comment"># _, pred = output.topk(maxk, 1, True, True)</span></span><br><span class="line">    <span class="comment"># output.shape为 [batch_size, category_count]，dim=1，所以我们为每个batch选择最大的类别数</span></span><br><span class="line">    <span class="comment"># 输入结果为[ batch_size,maxk]</span></span><br><span class="line">    <span class="comment"># topk返回结果的元组 (values, indexes) (值，索引)</span></span><br><span class="line">    <span class="comment"># 我们只需要索引(pred)</span></span><br><span class="line">    _, pred = output.topk(maxk, dim=<span class="number">1</span>, largest=<span class="literal">True</span>, <span class="built_in">sorted</span>=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 然后我们将索引转置为 [maxk, batch_size]</span></span><br><span class="line">    pred = pred.t()</span><br><span class="line">    correct = pred.eq(target.reshape(<span class="number">1</span>, -<span class="number">1</span>).expand_as(pred))</span><br><span class="line">    <span class="comment"># torch.eq对两个张量Tensor进行逐元素的比较，若相同位置的两个元素相同，则返回True；若不同，返回False</span></span><br><span class="line">    <span class="comment"># 将target展平，并将target扩展成类似于pred</span></span><br><span class="line">    <span class="comment"># target [batch_size] 变成 [1,batch_size]</span></span><br><span class="line">    <span class="comment"># target [1,batch_size] 通过广播？重复相同的类maxk次，从 [1,batch_size] 变为 [maxk, batch_size]</span></span><br><span class="line">    <span class="comment"># 当将索引(pred)与扩展后的target比较时，即torch.eq，会得到形状为 [maxk, batch_size] 的矩阵correct</span></span><br><span class="line">    <span class="string">""" correct=([[0, 0, 1,  ..., 0, 0, 0],</span></span><br><span class="line"><span class="string">         [1, 0, 0,  ..., 0, 0, 0],</span></span><br><span class="line"><span class="string">         [0, 0, 0,  ..., 1, 0, 0],</span></span><br><span class="line"><span class="string">         [0, 0, 0,  ..., 0, 0, 0],</span></span><br><span class="line"><span class="string">         [0, 1, 0,  ..., 0, 0, 0]], device='cuda:0', dtype=torch.uint8) """</span></span><br><span class="line"></span><br><span class="line">    res = []</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> topk:</span><br><span class="line">        correct_k = correct[:k].reshape(-<span class="number">1</span>).<span class="built_in">float</span>().<span class="built_in">sum</span>(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># correct[:k]：[maxk, batch_size] -&gt; [k, batch_size]</span></span><br><span class="line">        <span class="comment"># .reshape(-1)：[k, batch_size] -&gt; [k*batch_size]</span></span><br><span class="line">        <span class="comment"># .sum(0)：[k*batch_size] -&gt; [1]</span></span><br><span class="line">        res.append(correct_k.mul_(<span class="number">100.0</span> / batch_size))</span><br><span class="line">        <span class="comment"># mul 乘法</span></span><br><span class="line">        <span class="comment"># 所有带_都是inplace，意思就是操作后，原数也会改动</span></span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>topk函数：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) -&gt; (Tensor, LongTensor)</span><br></pre></td></tr></table></figure>
<p>input (Tensor)：输入张量，一个tensor数据 k
(int)：指明是得到前k个数据以及其index dim (int, optional)：
指定在哪个维度上排序， 默认是最后一个维度 largest (bool,
optional)：如果为True，按照大到小排序； 如果为False，按照小到大排序
sorted (bool, optional) ：控制返回值是否排序 out (tuple,
optional)：可选输出张量 (Tensor, LongTensor)</p>
<p>例如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.tensor([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">0</span>],</span><br><span class="line">        [ <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>],</span><br><span class="line">        [ <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>],</span><br><span class="line">        [ <span class="number">1</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>],</span><br><span class="line">        [ <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">1</span>]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(a.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line">torch.Size([<span class="number">5</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct_1 = a[:<span class="number">1</span>].reshape(-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(correct_1.shape)</span><br><span class="line"><span class="built_in">print</span>(correct_1)</span><br><span class="line">correct_1 = correct_1.<span class="built_in">float</span>().<span class="built_in">sum</span>(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(correct_1.shape)</span><br><span class="line"><span class="built_in">print</span>(correct_1)</span><br><span class="line">correct_1.mul_(<span class="number">100.0</span> / <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(correct_1.shape)</span><br><span class="line"><span class="built_in">print</span>(correct_1)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.Size([<span class="number">4</span>])</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">torch.Size([])</span><br><span class="line">tensor(<span class="number">2.</span>)</span><br><span class="line">torch.Size([])</span><br><span class="line">tensor(<span class="number">50.</span>) <span class="comment"># top_1 = 50%</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct_5 = a[:<span class="number">5</span>].reshape(-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(correct_5.shape)</span><br><span class="line"><span class="built_in">print</span>(correct_5)</span><br><span class="line">correct_5 = correct_5.<span class="built_in">float</span>().<span class="built_in">sum</span>(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(correct_5.shape)</span><br><span class="line"><span class="built_in">print</span>(correct_5)</span><br><span class="line">correct_5.mul_(<span class="number">100.0</span> / <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(correct_5.shape)</span><br><span class="line"><span class="built_in">print</span>(correct_5)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.Size([<span class="number">20</span>])</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">torch.Size([])</span><br><span class="line">tensor(<span class="number">4.</span>)</span><br><span class="line">torch.Size([])</span><br><span class="line">tensor(<span class="number">100.</span>) <span class="comment"># top_5 = 100%</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>基于人工标记的深度卷积神经网络在高光谱图像分类中的有效训练</title>
    <url>/article/202301319d7ab8f6.html</url>
    <content><![CDATA[<h1 id="基于人工标记的深度卷积神经网络在高光谱图像分类中的有效训练">基于人工标记的深度卷积神经网络在高光谱图像分类中的有效训练</h1>
<h2 id="主要问题">主要问题</h2>
<ol type="1">
<li>虽然深度学习神经网络（DLNN）可以获得非常好的精度分数，但它的缺点是需要大量的训练数据来估计模型参数。这样的数据并不总是可用的，因为通常一个高光谱图像只有少量训练标签可用。</li>
<li>迁移学习可以解决训练样本数量少的问题，但是这一策略并不直接适用于高光谱图像，因为通常情况下只有一种特定类型或特征的图像可用。</li>
</ol>
<h2 id="思路">思路</h2>
<p>对于DLNN分类，缺乏大量的训练数据是一个严重的复杂问题，因为它们通常需要大量数据才能实现高效率。DLNN在HSI分类中的最佳使用将需要仅用几个标记样本来学习它们。这可以通过搜索针对特定任务的定制良好的体系结构来获得，然而，这种方法需要相对较大的验证集才能获得有意义的结果。一种方法是扩大可用的训练集。这可以通过人工增加训练集或使用不同的数据集作为预训练的来源来实现。另一种方法是增加正则化步骤，以提高有限训练样本数量的泛化能力。在MugNet网络中采用了用于分类的网络体系结构的简化，其中训练样本很少。最后，在可能的情况下，使用转移学习方法。
迁移学习使用来自两个领域的训练样本，这两个领域具有共同的特征。网络首先在第一个域上进行预训练，该域有充足的训练样本供应，但不能解决手头的问题。随后，用第二个域更新训练，使权重适应实际问题。</p>
<h2 id="本文方法">本文方法</h2>
<p>为了缩小数据效率低下的深度学习模型与HSI实际应用之间的差距，我们提出了一种利用HSI图像上大量未标记数据点的方法。准确地说，我们提出了一个假设：可以利用未标记数据点的空间相似性来获得高光谱分类的准确性。为了证实我们的假设，我们构建了一个简单的空间聚类方法，该方法根据图像上的每个像素的空间位置为其分配人工标签。利用该人工数据集对深度学习分类器进行预训练。接下来，使用原始数据集对模型进行微调。通过一系列实验，我们证明了该方法优于标准的学习过程。我们的方法是由两个已知的现象驱动的:聚类假设和类中噪声的正则化效应。我们注意到，许多遥感图像具有共同的属性，最值得注意的是“聚类假设”——彼此接近或形成不同的聚类或组的像素经常共享类标签。此外，由于我们的聚类方法的简单形式，我们有目的地在预训练阶段使用的标签中引入噪声；只要正确标记的示例数量按比例缩放，这个标签噪声对最终的精度几乎没有影响。
我们的方法适用于以下情况：</p>
<ol type="1">
<li>高光谱遥感图像像素的分类；</li>
<li>神经网络用作分类器；</li>
<li>可用的培训标签很少。</li>
</ol>
<p>在这种情况下，我们建议用一个使用人工标签的预训练步骤来增强训练，这是独立于训练标签的。这一预训练步骤的加入可以看作是迁移学习方法的一种改进。在这种情况下，传统的迁移学习将使用具有大量标签的相关数据集(源域)进行预训练，然后使用当前数据集(目标域)进行微调。在我们的例子中，源域由高光谱图像中的每个点组成，而目标域仅由标记的样本组成。</p>
<span id="more"></span>
<h2 id="人工标记方法">人工标记方法</h2>
<p>我们为预训练步骤创建人工标签的方法是一种简单的分割算法，该算法假设样本光谱特征的局部同质性。它的工作原理是将考虑的图像分为<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.179ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 521 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g></svg></mjx-container></span>个矩形，其中每个矩形都有自己的标签。对于高<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.303ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 576 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g></g></g></svg></mjx-container></span>宽<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.62ex" height="1.027ex" role="img" focusable="false" viewBox="0 -443 716 454"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g></g></g></svg></mjx-container></span>的图像，我们将其高分为<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.986ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 878 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container></span>个大致相等的部分，将其宽分为<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 600 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container></span>个大致相等的部分，使<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="9.174ex" height="1.756ex" role="img" focusable="false" viewBox="0 -694 4055 776"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mo" transform="translate(798.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(1854.6,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(2954.8,0)"><path data-c="B7" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mi" transform="translate(3455,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container></span>。然后我们得到<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.179ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 521 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g></svg></mjx-container></span>个矩形，其中每个矩形的高约等于<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="4.421ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1954 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(576,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path></g></g><g data-mml-node="mi" transform="translate(1076,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container></span>，而宽度约等于<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="4.109ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1816 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(716,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path></g></g><g data-mml-node="mi" transform="translate(1216,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container></span>。每个矩形用不同的标签定义一个不同的人工类。图1给出了一个示意图。</p>
<img src="/article/202301319d7ab8f6/image-20230130210123748.png" class="" title="image-20230130210123748">
<p>人工标签的功能是让网络学习数据中存在的与类无关的blob模式。这将网络训练的重点放在实际训练标签的微调上，而网络“面向”当前图像的特征。当一个类由多个blob组成，并且并非所有的blob在训练集中都有样本的情况下，它也是很有优势的。在这种情况下，仅使用训练样本不太可能获得足够正确的标签[62]，但提出的网格结构迫使网络估计整个图像的特征。这种方法的另一个优点是将潜在耗时的预训练从专家标记时刻转移到习得时刻。换句话说，网络培训不需要等到专家的标签可用，而是可以在图像记录后立即开始。</p>
<h2 id="实验">实验</h2>
<ol type="1">
<li>实验1使用不同的高光谱图像和神经网络结构对所提出的方法进行了评估，以证明其鲁棒性。</li>
<li>实验2调查了人工标记中使用的补丁大小和形状引入的可变性。</li>
<li>实验3我们测试了这样的假设，即划分更多的patch比划分更少的patch产生更好的预训练集。我们使用一张特别设计的高光谱测试图像来研究这一点。</li>
<li>实验4我们检查了第2.2节中关于在神经网络训练中使用提出的带有噪声标签的人工标签方案出现的数据依赖表示的说法。</li>
</ol>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>用于高光谱图像分类的折叠谱生成对抗网络</title>
    <url>/article/20230120c94939a8.html</url>
    <content><![CDATA[<h1 id="用于高光谱图像分类的折叠谱生成对抗网络">用于高光谱图像分类的折叠谱生成对抗网络</h1>
<h2 id="摘要">1. 摘要</h2>
<p>本文提出了一种新的基于生成对抗网络(GAN)和折叠谱(FS-GAN)的半监督方法。具体来说，将原始光谱向量折叠为二维正方形光谱作为GAN的输入，可以生成光谱纹理，并在相邻和非相邻光谱波段上提供更大的感受野，用于深度特征提取。然后将生成的假折叠谱、标记和未标记的真实折叠谱输入到鉴别器中进行半监督学习。采用特征匹配策略防止模型崩溃。</p>
<h2 id="本文工作">2. 本文工作</h2>
<ol type="1">
<li>提出了一种新的基于半监督GAN的HSI分类算法，该算法结合了折叠谱和端到端半监督GAN模型。折叠谱可以挖掘谱域中的纹理信息，而端到端半监督GAN模型提供了最好的未标记样本。</li>
<li>采用特征匹配策略来稳定GAN的训练过程</li>
</ol>
<span id="more"></span>
<h2 id="本文方法">3. 本文方法</h2>
<h3 id="折叠光谱">3.1 折叠光谱</h3>
<h4 id="思路">3.1.1 思路</h4>
<p>空间相关性已在传统和基于 DL 的 HSI
算法中得到了充分的利用，但研究人员较少考虑谱域中的大量信息。此外，使用
CNN 作为基本模型的基于 DL
的算法利用卷积核作为特征提取的函数，因此它们的模型主要关注固定感受野中的特征。我们设计了一种折叠谱策略，可以直接将一维谱向量转换为二维图像，然后充分利用卷积核在谱域中提取特征的能力。</p>
<h4 id="方法">3.1.2 方法</h4>
<p>折叠光谱有两个优点：</p>
<p>1.折叠光谱可以从具有正方形形状的折叠光谱中提取光谱域中的纹理特征。选择折叠光谱曲线而不是仅仅重塑的原因是，重塑会破坏光谱向量的拓扑结构，而折叠光谱可以增强折叠光谱的边缘连续性。</p>
<img src="/article/20230120c94939a8/image-20230120211600109.png" class="" title="image-20230120211600109">
<p>如图1所示，长度为225的光谱向量可以转换为15*15的折叠光谱。这样，可以像HSI的空间纹理特征一样，清晰地找到2D折叠频谱中的光谱纹理特征。然后，我们可以使用这些特征来辅助HSI分类。此外，我们选择正方形以获得折叠光谱的最佳形状。<strong>由于正方形在高度和宽度上都比其他矩形具有相等和充分的纹理信息，因此具有更多的光谱纹理特征</strong>，当光谱向量的长度不能折叠成正方形时，我们选择以相反的顺序从最后一个波段扩展光谱向量。遵循这一规则，我们确保正方形折叠频谱可以进入生成对抗网络，而不会丢失频谱信息并破坏频谱向量拓扑。</p>
<p>2.与原始谱向量相比，更多的不连续谱带现在位于相邻位置。当通过卷积核从折叠的光谱中提取特征时，这些远带将位于相同的感受野中。</p>
<img src="/article/20230120c94939a8/image-20230120214023604.png" class="" title="image-20230120214023604">
<p>如图2所示，我们比较了1D谱向量和2D折叠谱之间的感受野差异。一维光谱矢量中的感受野只能覆盖一小范围的光谱，而二维折叠光谱可以覆盖整个正方形。因此，通过采用折叠光谱，借助于更大的感受野，可以发现更多的光谱信息。</p>
<h3 id="折叠频谱生成对抗网络fs-gan">3.2
折叠频谱生成对抗网络（FS-GAN）</h3>
<h4 id="思路-1">3.2.1 思路</h4>
<p>传统的GAN由两个子模型组成：生成器和鉴别器。这两个模型使用极小极大博弈来学习生成分布，并进一步匹配真实数据分布。生成器通过变换噪声变量来产生样本，并且鉴别器区分样本是来自真实数据还是生成器。通过交替训练生成器和鉴别器，生成器将输出真实的样本并且无法区分生成数据和真实数据。</p>
<p>受GAN的启发，我们设计了一种折叠频谱（FS-GAN），它可以实现HSI端到端半监督分类。FS-GAN的发生器和鉴别器都是从CNN修改而来的，以适应折叠频谱的设计。可以总结出FS-GAN的两个主要改进。首先，FS-GAN不再使用1D神经网络，而是应用2D神经网络来生成和区分折叠频谱。其次，我们设计了一个端到端的模型，没有预训练和微调过程。基于上述改进，可以在更简单的网络结构中提高分类性能。</p>
<h4 id="方法-1">3.2.2 方法</h4>
<img src="/article/20230120c94939a8/image-20230120214831066.png" class="" title="image-20230120214831066">
<p>图三为FS-GAN框架，我们可以看到生成器接收到噪声<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="4.914ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2171.8 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D70C" d="M58 -216Q25 -216 23 -186Q23 -176 73 26T127 234Q143 289 182 341Q252 427 341 441Q343 441 349 441T359 442Q432 442 471 394T510 276Q510 219 486 165T425 74T345 13T266 -10H255H248Q197 -10 165 35L160 41L133 -71Q108 -168 104 -181T92 -202Q76 -216 58 -216ZM424 322Q424 359 407 382T357 405Q322 405 287 376T231 300Q217 269 193 170L176 102Q193 26 260 26Q298 26 334 62Q367 92 389 158T418 266T424 322Z"></path></g><g data-mml-node="mi" transform="translate(550,-150) scale(0.707)"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g></g><g data-mml-node="mo" transform="translate(928.8,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1317.8,0)"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mo" transform="translate(1782.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span>作为输入，而鉴别器接收标记和未标记的
HSI
折叠谱，结合生成器提供的生成折叠谱作为输入。鉴别器的最后一层不再是只区分测试样本真伪的二进制分类器。相反，我们选择<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="5.908ex" height="1.731ex" role="img" focusable="false" viewBox="0 -683 2611.4 765"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g><g data-mml-node="mo" transform="translate(1111.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(2111.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container></span>分类器，其中<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.011ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 889 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g></g></svg></mjx-container></span>是类的数量，另一类是假样本的类。只要我们将折叠谱输入到模型中，就可以实现来自分类器的<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="5.908ex" height="1.731ex" role="img" focusable="false" viewBox="0 -683 2611.4 765"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g><g data-mml-node="mo" transform="translate(1111.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(2111.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container></span>的逻辑向量<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.471ex;" xmlns="http://www.w3.org/2000/svg" width="10.432ex" height="2.041ex" role="img" focusable="false" viewBox="0 -694 4611 902"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mn" transform="translate(331,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(734.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(1179.2,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(1623.9,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(2068.6,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(2513.2,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(2957.9,0)"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="TeXAtom" transform="translate(331,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mo" transform="translate(521,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(1299,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></g></g></svg></mjx-container></span>。FS-GAN的损失函数<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.541ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 681 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g></g></g></svg></mjx-container></span>包括两部分，监督损失<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.65ex;" xmlns="http://www.w3.org/2000/svg" width="9.321ex" height="2.195ex" role="img" focusable="false" viewBox="0 -683 4119.9 970.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="TeXAtom" transform="translate(714,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(469,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1041,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(1544,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(2010,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2461,0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mi" transform="translate(2946,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(3291,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(3760,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(4226,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g></g></g></svg></mjx-container></span>由预测类<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="10.761ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4756.4 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D70C" d="M58 -216Q25 -216 23 -186Q23 -176 73 26T127 234Q143 289 182 341Q252 427 341 441Q343 441 349 441T359 442Q432 442 471 394T510 276Q510 219 486 165T425 74T345 13T266 -10H255H248Q197 -10 165 35L160 41L133 -71Q108 -168 104 -181T92 -202Q76 -216 58 -216ZM424 322Q424 359 407 382T357 405Q322 405 287 376T231 300Q217 269 193 170L176 102Q193 26 260 26Q298 26 334 62Q367 92 389 158T418 266T424 322Z"></path></g><g data-mml-node="TeXAtom" transform="translate(550,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(878,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(1363,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(1883,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(2349,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g></g><g data-mml-node="mo" transform="translate(2471.7,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2860.7,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(3432.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(3877.4,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(4367.4,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span>和有标签折叠谱<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="9.628ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4255.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D70C" d="M58 -216Q25 -216 23 -186Q23 -176 73 26T127 234Q143 289 182 341Q252 427 341 441Q343 441 349 441T359 442Q432 442 471 394T510 276Q510 219 486 165T425 74T345 13T266 -10H255H248Q197 -10 165 35L160 41L133 -71Q108 -168 104 -181T92 -202Q76 -216 58 -216ZM424 322Q424 359 407 382T357 405Q322 405 287 376T231 300Q217 269 193 170L176 102Q193 26 260 26Q298 26 334 62Q367 92 389 158T418 266T424 322Z"></path></g><g data-mml-node="TeXAtom" transform="translate(550,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(520,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(1049,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(1410,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1971.1,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2360.1,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(2932.1,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(3376.7,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(3866.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span>交叉熵得到，无监督损失<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.65ex;" xmlns="http://www.w3.org/2000/svg" width="11.196ex" height="2.195ex" role="img" focusable="false" viewBox="0 -683 4948.7 970.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="TeXAtom" transform="translate(714,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(572,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1172,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(1641,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2213,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(2716,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(3182,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(3633,0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mi" transform="translate(4118,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(4463,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(4932,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(5398,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g></g></g></svg></mjx-container></span>是根据未标记的<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="7.514ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3321.1 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D70C" d="M58 -216Q25 -216 23 -186Q23 -176 73 26T127 234Q143 289 182 341Q252 427 341 441Q343 441 349 441T359 442Q432 442 471 394T510 276Q510 219 486 165T425 74T345 13T266 -10H255H248Q197 -10 165 35L160 41L133 -71Q108 -168 104 -181T92 -202Q76 -216 58 -216ZM424 322Q424 359 407 382T357 405Q322 405 287 376T231 300Q217 269 193 170L176 102Q193 26 260 26Q298 26 334 62Q367 92 389 158T418 266T424 322Z"></path></g><g data-mml-node="TeXAtom" transform="translate(550,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(520,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(1049,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(1410,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1971.1,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2360.1,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(2932.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span>和生成的折叠谱<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="4.59ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2029 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mo" transform="translate(786,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1175,0)"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mo" transform="translate(1640,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span>分别分为K个真类和一个假类的概率<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.489ex;" xmlns="http://www.w3.org/2000/svg" width="5.592ex" height="1.489ex" role="img" focusable="false" viewBox="0 -442 2471.7 658"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D70C" d="M58 -216Q25 -216 23 -186Q23 -176 73 26T127 234Q143 289 182 341Q252 427 341 441Q343 441 349 441T359 442Q432 442 471 394T510 276Q510 219 486 165T425 74T345 13T266 -10H255H248Q197 -10 165 35L160 41L133 -71Q108 -168 104 -181T92 -202Q76 -216 58 -216ZM424 322Q424 359 407 382T357 405Q322 405 287 376T231 300Q217 269 193 170L176 102Q193 26 260 26Q298 26 334 62Q367 92 389 158T418 266T424 322Z"></path></g><g data-mml-node="TeXAtom" transform="translate(550,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(878,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(1363,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(1883,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(2349,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g></g></g></g></svg></mjx-container></span>设计的。
在此基础上，FS-GAN的损失函数为：</p>
<img src="/article/20230120c94939a8/image-20230120231144192-16742275049231.png" class="" title="image-20230120231144192">
<p>半监督GAN的训练过程不稳定，容易分解。为了解决这些问题，我们采用了特征匹配策略，而不是简单地最大化鉴别器的输出。这种特征匹配策略将通过设置生成器中的不动点来使生成的数据与真实数据分布匹配。固定点是根据真实数据计算的，因此可以准确地描述真实数据的分布。如图
4 所示，我们选择隐藏层<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="4.299ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1900 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(550,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(939,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(1511,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span>的输出作为不动点，这意味着判别器不会过度训练，FS-GAN
模型会变得稳定。</p>
<img src="/article/20230120c94939a8/image-20230120231251740.png" class="" title="image-20230120231251740">
<p>FS-GAN的更新损失函数如下：</p>
<img src="/article/20230120c94939a8/image-20230120231417367.png" class="" title="image-20230120231417367">
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
</search>
