<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha256-yIDrPSXHZdOZhAqiBP7CKzIwMQmRCJ8UeB8Jo17YC4o=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"blog.tongorz.me","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.19.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"fold":{"enable":true,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="AhTong&#39;s blog">
<meta property="og:url" content="https://blog.tongorz.me/index.html">
<meta property="og:site_name" content="AhTong&#39;s blog">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Ahtong">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://blog.tongorz.me/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>AhTong's blog</title>
  



  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' data-cf-beacon='{&quot;token&quot;: &quot;5e61404f62704823b545b65cc244544a&quot;}'></script>





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">AhTong's blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">但盼风雨来,能留你在此</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">5</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">2</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">36</span></a></li><li class="menu-item menu-item-resources"><a href="/resources/" rel="section"><i class="fa fa-download fa-fw"></i>资源</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Ahtong"
      src="/images/81060761_p0.jpg">
  <p class="site-author-name" itemprop="name">Ahtong</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">36</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://blog.tongorz.me/article/202406255854198f.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/81060761_p0.jpg">
      <meta itemprop="name" content="Ahtong">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AhTong's blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | AhTong's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/article/202406255854198f.html" class="post-title-link" itemprop="url">基于多任务生成对抗网络的高光谱图像分类</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-06-25 21:10:31" itemprop="dateCreated datePublished" datetime="2024-06-25T21:10:31+08:00">2024-06-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-07-11 19:07:36" itemprop="dateModified" datetime="2024-07-11T19:07:36+08:00">2024-07-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">论文笔记</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>11 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>原文：《Classification of Hyperspectral Images via Multitask
Generative Adversarial Networks》</p>
<h2 id="摘要">摘要</h2>
<p>深度学习在高光谱图像（HSI）分类领域显示出巨大的潜力。然而，大多数深度学习模型严重依赖于可用训练样本的数量。在本文中，我们提出了一种多任务生成对抗网络（MTGAN），通过利用未标记样本的丰富信息来缓解这个问题。具体来说，我们设计了一个生成器网络来同时执行两个任务：重建任务和分类任务。前者旨在重建输入高光谱立方体，包括标记和未标记的立方体，而后者的任务试图识别立方体的类别。同时，我们构建了一个鉴别器网络来区分来自真实分布或重构样本的输入样本。通过对抗性学习方法，生成器网络将产生类似真实的立方体，从而间接提高分类任务的辨别能力和泛化能力。更重要的是，为了充分探索浅层的有用信息，我们在重建和分类任务中采用了跳跃连接。所提出的
MTGAN 模型在三个标准 HSI
上实现，实验结果表明它能够实现比其他最先进的深度学习模型更高的性能。</p>
<h2 id="研究思路">研究思路</h2>
<p>在本文中，我们提出了一种用于HSI光谱空间分类的多任务GAN（MTGAN）。在MTGAN中，生成器同时承担两个不同的任务：重构任务和分类任务。重构任务的目标是利用编码器子网和解码器子网重构输入HSI多维数据集，分类任务的目标是通过CNN识别输入多维数据集的类别。分类任务中CNN的第一层卷积层与重构任务中的编码器子网共享相同的结构。MTGAN的鉴别器由另一个CNN构建，该CNN的输入是真实的HSI立方体或由生成器重建的HSI立方体。在训练过程中，鉴别器试图准确区分真实的HSI立方体和重建的HSI立方体，而生成器则试图通过重建尽可能真实的立方体来欺骗鉴别器。经过这种对抗性训练，生成器能够重建类似真实的立方体，这表明编码器子网捕获了HSI立方体的内在表示。由于分类任务共享编码器子网的结构，其分类能力也将得到提高。
与现有的基于GAN的HSI分类模型相比，我们提出的MTGAN模型至少有两个优点。首先，与[36]和[37]不同的是，MTGAN中的生成器可以在对抗性训练后直接应用于HSI分类，而不需要训练额外的分类器，这也可能花费大量时间。其次，MTGAN能够从输入的HSI立方体中同时充分学习光谱和空间特征，这对于HSI光谱-空间分类至关重要。
本文贡献如下：</p>
<ol type="1">
<li>我们提出了一种用于HSI光谱空间分类的MTGAN。MTGAN中的生成器和鉴别器都是基于CNN的。该生成器可以同时对输入数据集进行重构和分类。得益于重构过程，未标记样本被充分利用来提高CNN的分类性能。</li>
<li>我们在重构任务和分类任务中都采用了跨层连接。通过这些连接，可以传递空间信息和浅层的判别信息，分别辅助重建和分类任务。</li>
<li>我们在三个具有挑战性的HSI数据集上测试了提出的MTGAN模型：Indian Pines
2010、Houston 2013和Houston
2018。在所有这些模型上，MTGAN都能够获得比所比较的深度学习模型更高的性能，这充分验证了其有效性。</li>
</ol>
<h2 id="本文方法">本文方法</h2>
<h3 id="mtgan的结构">MTGAN的结构</h3>

<p>我们提出的MTGAN模型尝试在GAN中加入多任务学习框架，可以充分利用未标记的样本来提高CNN的分类性能。如图2所示，MTGAN的生成器包含三个模块：编码器模块、解码器模块和分类器模块。生成器的输入是每个像素周围裁剪的小立方体，而不是随机噪声。将编码器和解码器模块组合在一起重建输入立方体，而编码器和分类器模块用于对输入立方体进行分类。这两个任务共享编码器模块，使它们相互促进。MTGAN的鉴别器由一个CNN构成，期望能区分真实立方体和重建立方体。下面，我们将详细介绍产生器和鉴别器的结构。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/article/202406255854198f.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://blog.tongorz.me/article/20240225c2afc392.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/81060761_p0.jpg">
      <meta itemprop="name" content="Ahtong">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AhTong's blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | AhTong's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/article/20240225c2afc392.html" class="post-title-link" itemprop="url">基于双流类自适应网络的半监督高光谱图像分类</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-02-25 22:14:35" itemprop="dateCreated datePublished" datetime="2024-02-25T22:14:35+08:00">2024-02-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-03-01 00:10:10" itemprop="dateModified" datetime="2024-03-01T00:10:10+08:00">2024-03-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">论文笔记</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>17 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>原文：《Dual-Stream Class-Adaptive Network for Semi-Supervised
Hyperspectral Image Classification》</p>
<h2 id="摘要">摘要</h2>
<p>半监督遥感高光谱图像（HSI）分类旨在利用有标签和无标签样本进行准确的土地覆盖识别。然而，数据分布不平衡和不同的分类难度会对分类性能产生负面影响。针对这个问题，本文提出了一种用于半监督HSI分类的新型双流类自适应网络（DSCA-Net）。首先，引入了一个超像素引导有标签传播（superpixel-guided
label propagation,
SGLP）模块来减轻数据分布不平衡的负面影响。具体来说，通过超像素级别的相似性度量和有标签传播来实现对无标签样本有标签的近似估计，从而对每个类别应用相同的采样。然后，构建了一个基于一致性正则化的双流网络，该网络共享相同的编码器来表示有标签或无标签样本的特征。在此基础上，设计了两个不同的分类器来强制对同一无标签样本的不同增强版本进行类似的预测，从而允许无标签样本以监督方式训练模型。最后，由于不同的类别总是有着不同的学习难度，相同的处理方式可能会导致“易”类过拟合和“难”类预测偏差。与传统使用固定阈值选择无标签样本不同，本文根据模型的学习状态计算动态的类自适应阈值。通过这种方式，为“易”类分配更高的阈值以减少样本冗余，并为“难”类设置更低的阈值以选择更多样本。实验结果证明了所提出方法的有效性和优越性。代码可在https://github.com/luting-hnu/DSCA-Net上获得。</p>
<h2 id="主要思路">主要思路</h2>
<p>当训练样本不足时，监督学习的性能会受到负面影响。为了减少对有标签样本的依赖，一些深度学习方法开始探索在高光谱图像
(HSI) 分类任务中使用半监督学习（SSL）策略 [23, 24, 25,
26]。具体来说，SSL 方法通过同时利用有标签和无标签样本，来提升性能
[27]。SSL
方法可以大致分为三大类：基于生成式模型、基于图论方法和自训练方法。其中，生成式模型旨在通过自编码器（AE）模型获取无标签样本的上下文信息。例如，Zhou
等人 [28] 分别使用两个堆叠的自编码器对 HSI
的光谱和空间特征进行预训练，然后使用 SSL
共训练的方式扩大了初始训练集。Jia 等人 [29]
分别集成了一个自编码器模块和一个孪生网络，用于挖掘大量无标签数据中的信息，然后利用有限的有标签样本集进行微调。基于图的方法通过捕获有标签和无标签样本共同揭示的内在结构来实现分类目标。Kotzagiannidis
和 Schönlieb [30] 提出了一种用于高光谱图像分类的多级边缘高效 SSL
图形网络，该网络通过在图构建中嵌入伪标签特征来利用有标签样本。Xi 等人
[31]
提出了一种新的跨尺度图原型层，以增强半监督高光谱图像分类的节点特征和原型的判别力和代表性。自训练方法则需要迭代更新高可信度的无标签样本（即伪标签），并与少量有标签样本结合来训练分类器
[32]。Feng 等人 [33]
提出了一种半监督长尾学习方法，该方法基于空间邻域信息确定无标签样本的有标签，并使用新的标准重新确定它们以提高伪标签的准确性。然而，生成式模型需要适当的约束来生成高度可信的伪标签样本，这在实践中很难满足。基于图的方法可以很好地构建有标签和无标签样本之间的连接用于有标签传播，但这通常需要大量计算。相比之下，自训练方法由于其简单原理和良好的性能而被广泛研究。
基于以上分析，我们本文尝试设计一种基于 CNN
模型和自训练方法的新型半监督深度学习方法。<strong>本文旨在解决两个主要问题，即数据分布不平衡和不同类别学习难度差异。</strong>在自训练方法中，需要随机选择一部分数据作为无标签样本集（ULS）。然而，由于数据分布总是存在不平衡性
[34]，数量较少的类别可能不会被选中，从而导致分类的偏差估计。此外，自训练格式为了选择高可信度的伪标签样本，需要设置一个较高的阈值，通常会设置为
0.95 等高值 [35]。然而，事实上，不同类别应该具有不同的学习难度
[36]。如下图 1(a)
所示，对所有伪标签样本设置相同的阈值，会导致对于学习难度高的类别，只有少量样本甚至没有样本能够超过如此高的阈值。相比之下，正如示意图
1(b) 所示，不同的类别需要具有动态的类适应型阈值。 <img src="/article/20240225c2afc392/image-20240228171505617.png" class="" title="image-20240228171505617">
针对数据分布不平衡和不同类别学习难度带来的负面影响，我们提出了一种用于半监督高光谱图像分类的新型双流类适应型网络（DSCA-Net）。首先，为了缓解数据分布不平衡的问题，我们提出了一种<strong>超像素引导有标签传播
(SGLP)
模块</strong>，该模块通过基于超像素的相似性测量和标签传播来传播已知标签，为后续训练提供类别平衡的无标签训练集。在过去的遥感图像分类研究中，基于超像素的技术被证明是非常有用的。例如，Zhao
等人 [37]
提出了一种超像素引导的可变形卷积方法，可以根据高光谱图像的空间结构自适应地提取特征。Jia
等人 [38]
则提出了一种无需任何参数调优的超像素级加权标签传播方法。之前的方法旨在在超像素层面实现连续迭代的特征学习或标签更新，但这需要大量的计算。与这些工作不同，我们提出的模块在网络训练之前只进行一次余弦相似度测量和标签传播，因此计算量较低。然后，通过一致性正则化，我们设计了两个不同的分类器，以强制对同一无标签样本的不同增强版本做出相似的预测，从而使无标签样本能够以监督方式训练模型。最重要的是，传统固定阈值方法（即为每个类别设置相同的阈值）存在不足之处
[39]。为此，我们提出了一种类别自适应动态阈值（CADT）策略。该策略根据模型的学习状态计算难度估计，并由此获得每个类别的对应阈值。此外，我们设计了一个非线性映射函数，将难度估计值映射到每个类别的最终自适应阈值。综上所述，我们方法的主要贡献如下：</p>
<ol type="1">
<li>我们提出了一种用于高光谱图像分类的新型双流半监督深度学习网络。具体来说，该网络中的两个流共享相同的编码器以获取相似的特征表示，并采用两个分类器进行不同的标签估计。通过联合优化监督和非监督交叉熵损失，网络模型可以学习来自有标签和无标签样本的更具判别性的特征。</li>
<li>为了减轻数据分布不平衡的负面影响，我们设计了一个 SGLP
模块，用于快速生成所有无标签像素的可能标签的近似估计。基于相似度测量，将有限的已知标签逐个超像素地进行传播。根据这一点，我们可以从每个类别中选择相同数量的无标签样本构建平衡的
ULS。</li>
<li>为了自适应地确定阈值以更好地选择具有高置信度的伪标签样本，这里提出了一种
CADT
策略。考虑到不同类别具有不同的学习难易度，我们评估它们的分类难度，并利用评估值来调整阈值。这有助于缓解“容易”类别的过度拟合和“困难”类别的偏差预测。</li>
</ol>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/article/20240225c2afc392.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://blog.tongorz.me/article/20240201d2354a7c.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/81060761_p0.jpg">
      <meta itemprop="name" content="Ahtong">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AhTong's blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | AhTong's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/article/20240201d2354a7c.html" class="post-title-link" itemprop="url">Hexo 配置 NexT 主题配置流程（自用）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-02-01 23:42:13" itemprop="dateCreated datePublished" datetime="2024-02-01T23:42:13+08:00">2024-02-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-02-03 15:49:31" itemprop="dateModified" datetime="2024-02-03T15:49:31+08:00">2024-02-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9D%82%E9%A1%B9/" itemprop="url" rel="index"><span itemprop="name">杂项</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>14 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="hexo-部署">Hexo 部署</h2>
<h3 id="docker-版本一键安装">docker 版本一键安装</h3>
<p>采用 <a target="_blank" rel="noopener" href="https://github.com/appotry">appotry</a>
的docker版本</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker create --name=hexo \</span><br><span class="line">  -e HEXO_SERVER_PORT=4000 \</span><br><span class="line">  -e GIT_USER=<span class="string">""</span> \</span><br><span class="line">  -e GIT_EMAIL=<span class="string">""</span> \</span><br><span class="line">  -v /mnt/hexo:/app \</span><br><span class="line">  -p 4000:4000 \</span><br><span class="line">  bloodstar/hexo</span><br></pre></td></tr></table></figure>
<h3 id="ssh-部署">SSH 部署</h3>
<blockquote>
<p><strong>Docker会自动随机生成ssh key</strong> 在 /app/.ssh 目录下面。
1.将<strong>SSH</strong> 公钥复制到剪贴板。
2.在任何页面的右上角，单击您的个人资料照片，然后单击Settings（设置）。
3.在用户设置侧边栏中，单击<strong>SSH</strong> and GPG
keys（<strong>SSH</strong> 和GPG 密钥）。 4.单击New <strong>SSH</strong>
key（新<strong>SSH</strong> 密钥）或Add <strong>SSH</strong>
key（添加<strong>SSH</strong> 密钥）。</p>
</blockquote>
<h3 id="运行">运行</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it hexo /bin/bash</span><br></pre></td></tr></table></figure>
<h3 id="常用命令">常用命令</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">hexo server <span class="comment">#启动本地服务器，用于预览主题。Hexo 会监视文件变动并自动更新，除修改站点配置文件外,无须重启服务器,直接刷新网页即可生效。</span></span><br><span class="line">hexo server -s <span class="comment">#以静态模式启动</span></span><br><span class="line">hexo server -p 4000 <span class="comment">#更改访问端口 (默认端口为 5000，’ctrl + c’关闭 server)</span></span><br><span class="line">hexo server -i IP地址 <span class="comment">#自定义 IP</span></span><br><span class="line">hexo clean <span class="comment">#清除缓存 ,网页正常情况下可以忽略此条命令,执行该指令后,会删掉站点根目录下的 public 文件夹</span></span><br><span class="line">hexo g <span class="comment">#生成静态网页 (执行 $ hexo g后会在站点根目录下生成 public 文件夹, hexo 会将”/blog/source/“ 下面的.md 后缀的文件编译为.html 后缀的文件,存放在”/blog/public/ “ 路径下)</span></span><br><span class="line">hexo d <span class="comment">#自动生成网站静态文件，并将本地数据部署到设定的仓库(如 github)</span></span><br><span class="line">hexo init 文件夹名称 <span class="comment">#初始化 XX 文件夹名称</span></span><br><span class="line">npm update hexo -g<span class="comment">#升级</span></span><br><span class="line">npm install hexo -g <span class="comment">#安装</span></span><br><span class="line">node -v <span class="comment">#查看 node.js 版本号</span></span><br><span class="line">npm -v <span class="comment">#查看 npm 版本号</span></span><br><span class="line">git --version <span class="comment">#查看 git 版本号</span></span><br><span class="line">hexo -v <span class="comment">#查看 hexo 版本号</span></span><br><span class="line">hexo new page “music” <span class="comment">#新增页面music</span></span><br><span class="line">hexo new post “文章名称” <span class="comment">#新增文章</span></span><br></pre></td></tr></table></figure>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/article/20240201d2354a7c.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://blog.tongorz.me/article/20231212bb26f499.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/81060761_p0.jpg">
      <meta itemprop="name" content="Ahtong">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AhTong's blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | AhTong's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/article/20231212bb26f499.html" class="post-title-link" itemprop="url">超越自注意力：用于医学图像分割的可变形大核注意力</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-12-12 20:50:07" itemprop="dateCreated datePublished" datetime="2023-12-12T20:50:07+08:00">2023-12-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-02-06 23:12:02" itemprop="dateModified" datetime="2024-02-06T23:12:02+08:00">2024-02-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">论文笔记</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.9k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>6 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>原文：《Beyond Self-Attention: Deformable Large Kernel Attention for
Medical Image Segmentation》</p>
<h2 id="本文贡献">本文贡献</h2>
<ol type="1">
<li>为了解决上述挑战，我们提出了 DeformableLKA 模块 (❶)
形式的解决方案，它作为我们网络设计中的基本构建块。该模块被明确设计为有效处理上下文信息，同时保留局部特征。</li>
<li>我们的架构中两个方面之间的平衡增强了其实现精确语义分割的能力。值得注意的是，我们的模型引入了基于数据的感受野的动态适应，这与传统卷积运算中的传统固定滤波器掩模不同。这种自适应方法使我们能够克服与静态方法相关的固有限制。这种创新方法扩展到
D-LKA 网络架构 (❷) 的 2D 和 3D 版本的开发。对于 3D 模型，D-LKA
机制经过定制以适应 3D
环境，从而实现不同体积切片之间的无缝信息交换。</li>
<li>(❸) 最后，我们的贡献进一步强调了它的计算效率。我们通过完全依赖 D-LKA
概念的设计实现了这一目标，从而在各种细分基准上取得了卓越的性能，从而使我们的方法成为一种新的
SOTA 方法。</li>
</ol>
<h2 id="主要方法">主要方法</h2>
<p>在本节中，我们首先概述该方法。首先，我们回顾一下由 Guo 等人 [23]
提出的大核注意力（LKA）的概念。然后，我们介绍了我们对可变形 LKA
模块的创新探索。在此基础上，我们引入了用于分割任务的 2D 和 3D
网络架构。</p>
<h3 id="大核注意力lka">大核注意力（LKA）</h3>
<p>大卷积核提供与自注意力机制类似的感受野。通过使用深度卷积、深度扩张卷积和
<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="4.023ex" height="1.507ex" role="img" focusable="false" viewBox="0 -666 1778 666"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(500,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mn" transform="translate(1278,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container></span>
卷积，可以用更少的参数和计算来构造大的卷积核。对于 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="6.14ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 2714 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(888,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1666,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g></g></g></svg></mjx-container></span> 维输入和通道 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.719ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 760 727"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g></g></g></svg></mjx-container></span>，构造 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="5.783ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 2556 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g><g data-mml-node="mi" transform="translate(889,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1667,0)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g></g></svg></mjx-container></span> 核的深度卷积和深度膨胀卷积的核大小方程为： <img src="/article/20231212bb26f499/image-20231212162602091.png" class="" title="image-20231212162602091">
内核大小为 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.011ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 889 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g></g></svg></mjx-container></span>，膨胀率为 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.176ex" height="1.593ex" role="img" focusable="false" viewBox="0 -694 520 704"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g></svg></mjx-container></span>。参数数量 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="7.653ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3382.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g><g data-mml-node="mo" transform="translate(2029,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(2473.7,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(2993.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span> 和浮点运算（FLOPs）<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="7.649ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3380.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path></g><g data-mml-node="mo" transform="translate(749,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1138,0)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g><g data-mml-node="mo" transform="translate(2027,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(2471.7,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(2991.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span> 的计算公式如下： <img src="/article/20231212bb26f499/image-20231212162716650.png" class="" title="image-20231212162716650">
<img src="/article/20231212bb26f499/image-20231212162948538.png" class="" title="image-20231212162948538"> FLOPs
的数量随着输入图像的大小线性增长。参数数量随着通道数量和内核大小呈二次方增加。然而，由于两者通常都很小，因此它们不是限制因素。
为使核大小 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.011ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 889 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g></g></svg></mjx-container></span>
固定时的参数个数最少，可将式 3 对膨胀率 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.176ex" height="1.593ex" role="img" focusable="false" viewBox="0 -694 520 704"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g></svg></mjx-container></span> 的导数设为零： <img src="/article/20231212bb26f499/image-20231212163127897.png" class="" title="image-20231212163127897">
例如，当内核大小为 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="7.291ex" height="1.731ex" role="img" focusable="false" viewBox="0 -683 3222.6 765"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g><g data-mml-node="mo" transform="translate(1166.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(2222.6,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" transform="translate(500,0)"></path></g></g></g></svg></mjx-container></span> 时，结果
<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="8.216ex" height="1.62ex" role="img" focusable="false" viewBox="0 -694 3631.6 716"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(797.8,0)"><path data-c="2248" d="M55 319Q55 360 72 393T114 444T163 472T205 482Q207 482 213 482T223 483Q262 483 296 468T393 413L443 381Q502 346 553 346Q609 346 649 375T694 454Q694 465 698 474T708 483Q722 483 722 452Q722 386 675 338T555 289Q514 289 468 310T388 357T308 404T224 426Q164 426 125 393T83 318Q81 289 69 289Q55 289 55 319ZM55 85Q55 126 72 159T114 210T163 238T205 248Q207 248 213 248T223 249Q262 249 296 234T393 179L443 147Q502 112 553 112Q609 112 649 141T694 220Q694 249 708 249T722 217Q722 153 675 104T555 55Q514 55 468 76T388 123T308 170T224 192Q164 192 125 159T83 84Q80 55 69 55Q55 55 55 85Z"></path></g><g data-mml-node="mn" transform="translate(1853.6,0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" transform="translate(500,0)"></path><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z" transform="translate(778,0)"></path><path data-c="37" d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z" transform="translate(1278,0)"></path></g></g></g></svg></mjx-container></span>。将公式扩展到三维情况很简单，对于大小为
<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="9.774ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 4320 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(888,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1666,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(2714,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(3492,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g></g></g></svg></mjx-container></span> 和通道 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.719ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 760 727"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g></g></g></svg></mjx-container></span> 的输入，则参数数量 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="9.226ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4077.9 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="TeXAtom" transform="translate(675,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g><g data-mml-node="mi" transform="translate(500,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1446.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1835.2,0)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g><g data-mml-node="mo" transform="translate(2724.2,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(3168.9,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(3688.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span> 和 FLOPs <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="9.228ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4078.9 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path></g><g data-mml-node="TeXAtom" transform="translate(676,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g><g data-mml-node="mi" transform="translate(500,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1447.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1836.2,0)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g><g data-mml-node="mo" transform="translate(2725.2,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(3169.9,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(3689.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span> 的方程为： <img src="/article/20231212bb26f499/image-20231212163339199.png" class="" title="image-20231212163339199">
内核大小为 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.011ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 889 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g></g></svg></mjx-container></span> 和膨胀率为 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.176ex" height="1.593ex" role="img" focusable="false" viewBox="0 -694 520 704"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g></svg></mjx-container></span>。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/article/20231212bb26f499.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://blog.tongorz.me/article/2023121161de97aa.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/81060761_p0.jpg">
      <meta itemprop="name" content="Ahtong">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AhTong's blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | AhTong's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/article/2023121161de97aa.html" class="post-title-link" itemprop="url">卷积嵌入使分层视觉 Transformer 更强大</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-12-11 13:45:52" itemprop="dateCreated datePublished" datetime="2023-12-11T13:45:52+08:00">2023-12-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-02-06 23:06:19" itemprop="dateModified" datetime="2024-02-06T23:06:19+08:00">2024-02-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">论文笔记</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>17 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>原文：《Convolutional Embedding Makes Hierarchical Vision Transformer
Stronger》</p>
<h2 id="摘要">摘要</h2>
<p>视觉变换器（ViT）最近在一系列计算机视觉任务中占据主导地位，但在没有适当归纳偏差的情况下，它存在训练数据效率低和局部语义表示能力差的问题。卷积神经网络
(CNN) 本质上捕获区域感知语义，这启发了研究人员将 CNN 重新引入 ViT
架构中，为 ViT 提供理想的归纳偏差。然而，嵌入 ViT 中的微观 CNN
所实现的局部性是否足够好？本文中，我们通过深入探索混合 CNN/ViT
的宏观架构如何提升分层 ViT
的性能，来研究这个问题。特别的，我们研究了令牌嵌入层，别名卷积嵌入（CE）的作用，并系统地揭示了
CE 如何在 ViT 中注入理想的归纳偏差。 此外，我们将最优的 CE
配置应用于最近发布的 4 个最先进的
ViT，有效提升了相应的性能。最后，发布了一系列高效的混合 CNN/ViT（称为
CETNet），它们可以作为通用视觉主干。具体来说，CETNets 在 ImageNet-1K
上实现了84.9%的 Top-1准确率（从头开始训练），在 COCO
基准上实现了48.6%的box
mAP，在ADE20K上实现了51.6%的mIoU，大大提高了相应最先进基线的性能。</p>
<h2 id="主要思路">主要思路</h2>
<p>ViT
依靠高度灵活的多头自注意力层来优化动态注意力、捕获全局语义并实现良好的泛化能力。然而，最近的研究发现，由于缺乏适当的归纳偏差和局部性，ViT
的可优化性不合格 [59]、训练样本效率低
[15]，并且难以对图像中的复杂视觉特征和局部关系进行建模 [38,58]
。大多数现有的工作试图通过两种路径将局部机制引入到 ViT
中。其中一项工作通过非卷积方式缓解归纳偏差问题。liu 等人 [35,18,52]
将注意力计算限制在局部窗口中，使得注意力层具有局部感受野，使得整个网络仍然保持近乎纯粹的基于注意力的架构。同时，由于
CNN 固有的滑动窗口方式、局部感受野和归纳偏差
[2]，本质上是更高效的，因此另一项工作直接将 CNN 集成到 ViT 设计中，以硬
[58,33,64] 或软 [15,51] 的方式将卷积归纳偏差引入到 ViT
中。然而，大多数这些工作都集中在修改 ViT
的微观设计以实现局部性，这就引发了一个问题： <strong><em>通过 ViT
的微观设计获得的归纳偏差是否足够强大，足以为 ViT 赋予局部性？
或者网络的宏观架构设计能否进一步向 ViT
引入理想的归纳偏差？</em></strong>
我们根据以下发现提出上述问题。之前的工作 EarlyConv [59] 指出，简单地用 5
层卷积干替换原始 patchify 干可以在 ImageNet1K 上产生 1-2% 的 top-1
准确率，并提高 ViT 的训练稳定性。随后，CoAtNet [14] 根据观察进一步探索了
CNN 和 ViT
的混合设计：深度卷积可以自然地集成到注意力块中。同时，这些工作表明，卷积层可以有效地在整个网络的浅层引入归纳偏差。然而，当我们回顾现代
CNN 的发展路线图时，在2012年底，AlexNet [32]展示了 CNN
的潜力之后，后续的研究，例如 VGG [44]，ResNet [25]，DenseNet
[30]，EfficientNet [49,50]，ConvNet2020 [36]
等等，揭示了：<strong>即使在网络的深层，卷积也可以有效且高效地表示复杂的视觉特征</strong>。我们的研究探索了混合
CNN/ViT 的宏观网络设计。我们希望弥合纯 CNN 网络和纯 ViT
网络之间的差距，并扩展混合 CNN/ViT 网络的局限性。
为了检验这个假设，我们从 CNN 的有效感受野（ERF）开始。正如 Luo
等人的先前工作 [37] 指出的，CNN
的输出在很大程度上取决于它们的ERF。有了更大的 ERF，CNN
就不会遗漏任何重要信息，从而获得更好的预测结果或视觉特征。有了更大的感受野（ERF），CNN
就不会遗漏任何重要信息，从而获得更好的预测结果或视觉特征。在此视角下，我们的探索是通过网络架构的宏观设计，对注意力层施加强大且有效的归纳偏差。我们特别关注分层
ViT
架构的补丁嵌入，别名卷积嵌入（CE）。我们在每个阶段的开始都设置了卷积嵌入（CE），如图2所示。
<img src="/article/2023121161de97aa/image-20231209114519772.png" class="" title="image-20231209114519772"> CE
的目的是调整维度和令牌数量。大多数后续的工作也应用了一个或两个卷积嵌入层
[58,18,67,55,33]。然而，这些嵌入层无法提供足够的 ERF
来捕获具有理想归纳偏差的复杂视觉表示。由于堆叠更多的卷积层可以增加 ERF
[37]，我们构建了一个仅具有 1 层 CE 的简单基线，并逐渐增加 CE
中的卷积层数量以获得更多变体。同时，保持尽可能小的浮点运算 FLOPs
和参数数量变化。我们观察到，每个阶段 CE
的微小变化都会导致最终模型的性能显着提高。 基于大量实验，我们进一步了解
CE 如何通过注入所需的归纳偏差来影响 CNN/ViT
的混合网络设计。我们做出了一些观察。1) CNN
即使在网络的深层也能带来很强的归纳偏差，使得整个网络更容易训练，更容易捕获更复杂的视觉特征。同时，ViT
让整个网络拥有更高的泛化上限。2）CE可以施加有效的归纳偏差，但不同的卷积层表现出不同的有效性。此外，大的
ERF 对于设计 CE 或向 ViT 注入所需的归纳偏差至关重要，即使它是纯 CNN
网络中的传统设计 [50,37]。3) CNN 甚至可以帮助 ViT
在深度网络中看得更清楚，为指导如何设计混合 CNN/ViT
网络提供有价值的见解。4) 将宏观和微观相结合引入归纳偏差有利于获得基于
ViT 的网络的更高泛化上限。 我们的结果证明了卷积嵌入（CE)和深度混合
CNN/ViT 设计对于视觉任务的重要性。ViT 是 CNN 的通用版本
[10]，大量的工作已经证明了基于 ViT
的网络的高度泛化性，这激励研究人员纯注意力网络的性能上限。在发现归纳偏差对于显着提高
ViT 的训练速度和样本效率至关重要之后，人们的努力主要致力于创建 ViT
的微观设计以增强它 [35,15]。同时，EarlyConv [59] 和 CoAtNet [14]
验证了基于 ViT 的网络浅层卷积的效率。我们的研究进一步突破了混合 CNN/ViT
网络宏观设计的界限。我们的结果还表明，即使在 ViTs 网络的深层，正确选择
CNN/ViTs
设计的组合，也可以进一步改善整个网络的上限性能限制。最后，我们提出了一系列混合
CNN/ViT 模型作为通用视觉主干。
总而言之，我们希望本文中提出的发现和讨论能够为社区提供可能的见解，并鼓励人们重新思考
CE 在混合 CNN/ViT 网络设计中的价值。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/article/2023121161de97aa.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://blog.tongorz.me/article/202311291633467c.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/81060761_p0.jpg">
      <meta itemprop="name" content="Ahtong">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AhTong's blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | AhTong's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/article/202311291633467c.html" class="post-title-link" itemprop="url">基于 4D 卷积 Swin Transformer 的小样本分割代价聚合</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-11-29 22:31:38" itemprop="dateCreated datePublished" datetime="2023-11-29T22:31:38+08:00">2023-11-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-02-06 19:19:07" itemprop="dateModified" datetime="2024-02-06T19:19:07+08:00">2024-02-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">论文笔记</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>20 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>原文：《Cost Aggregation with 4D Convolutional Swin Transformer for
Few-Shot Segmentation》</p>
<h2 id="摘要">摘要</h2>
<p>本文提出了一种新颖的代价聚合网络，称为体积聚合
Transformer（VAT），用于小样本分割。Transformer
的使用可以通过对全局感受野的自注意力来促进相关图的聚合。然而，用于
Transformer
处理的相关图的标记化可能是有害的，因为标记边界处的不连续性减少了标记边缘附近可用的局部上下文并减少了归纳偏差。为了解决这个问题，我们提出了一种
4D 卷积 Swin Transformer，其中高维 Swin Transformer
之前是一系列小核卷积，这些小核卷积将局部上下文传递给所有像素并引入卷积归纳偏差。我们还通过在金字塔结构中应用
Transformer
来提高聚合性能，其中较粗级别的聚合引导更精细级别的聚合。然后，利用查询外观嵌入，在后续的解码器中过滤变压器输出中的噪声。该模型在小样本分割的所有标准基准测试中实现了新的最先进水平。研究表明，VAT
在语义对应方面取得了最先进的性能，其中代价聚合也起着核心作用。代码和训练模型可在
https://seokju-cho.github.io/VAT/ 获取。</p>
<h2 id="简介">简介</h2>
<p>语义分割是一项基本的计算机视觉任务，旨在用其相应的类别来标记图像中的每个像素。在深度神经网络和包含真实地物分割标注的大规模数据集的帮助下，这个方向已经取得了实质性进展
[37,46,3,4,61]。然而，手动标记按像素级划分的图需要大量的工作，因此很难添加新的类别。为了减少对标记数据的依赖，人们越来越关注小样本分割
[49, 55]，其中仅使用少量支持图像及其相关的掩膜来预测查询图像的分割。
小样本分割的关键是有效利用少量支持样本。许多工作尝试通过从样本中提取原型模型并将其用于与查询进行特征比较
[58,10,35,78]。然而，此类方法忽略了支持和查询特征之间的像素级成对关系或特征的空间结构，这可能导致次优结果。
为了解释这种关系，我们观察到小样本分割可以重新表述为语义对应，其目的是在语义相似的图像之间找到像素级对应，这些图像可能包含大量的类内外观和几何变化
[13,14,43]。最近的语义对应模型 [50,25,51,53,42,44,34,65,41]
遵循特征提取、代价聚合和流量估计的经典匹配流程
[54,47]。在代价聚合阶段，对匹配分数进行细化以产生更可靠的对应估计，这一阶段尤为重要，也是许多研究的重点
[53,42,52,22,34,29,41,6]。最近，CATs [6] 提出使用 Vision Transformer
[11]
进行代价聚合，但其对输入标记数量的二次复杂度限制了其适用性。它还忽略了匹配成本的空间结构，这可能会损害其性能。
在小样本分割领域，也存在尝试通过交叉注意力 [83] 或图注意力 [81,68,75]
利用成对信息来优化特征的方法。然而，他们仅依赖于原始相关图，而不聚合匹配分数。因此，他们的对应关系可能会因重复模式或背景混乱而产生歧义
[50,25,27,65,17]。为了解决这个问题，HSNet [40] 使用 4D
卷积聚合匹配分数，但其有限的感受野限制了远程上下文的聚合，并且由于使用固定内核，它缺乏适应输入内容的能力。
在本文中，我们介绍了一种新颖的代价聚合网络，体积聚合
Transformer（VAT），它通过提出的 4D 卷积 Swin Transformer
来解决小样本分割任务。具体来说，我们首先扩展 Swin Transformer [36]
及其补丁嵌入模块来处理高维相关图。通过引入 4D
卷积，进一步扩展了补丁嵌入模块，缓解了补丁嵌入带来的问题，即补丁边界附近有限的局部上下文和低归纳偏差。高维补丁嵌入模块被设计为一系列重叠的小核卷积，为每个像素带来局部上下文信息，并赋予卷积归纳偏差。为了进一步提高性能，我们采用金字塔结构来构建我们的架构，该结构将较粗级别的聚合相关图作为更精细级别的附加输入，从而提供分层指导。然后，我们的亲和力感知解码器以利用查询的外观嵌入给出的更高分辨率空间结构的方式对聚合匹配分数进行细化，并最终输出分割掩膜预测。
我们在几个基准测试中证明了我们的方法的有效性
[55,31,30]。我们的工作在小样本分割甚至语义对应的所有基准上都达到了最先进的性能，突出了代价聚合对于这两项任务的重要性，并显示了其在一般匹配方面的潜力。我们还进行了消融研究来证明我们的设计选择的合理性。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/article/202311291633467c.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://blog.tongorz.me/article/202311222df0ba87.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/81060761_p0.jpg">
      <meta itemprop="name" content="Ahtong">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AhTong's blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | AhTong's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/article/202311222df0ba87.html" class="post-title-link" itemprop="url">基于多尺度 3D 空洞卷积 Swin Transformer 的高光谱图像分类</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-11-22 22:48:14" itemprop="dateCreated datePublished" datetime="2023-11-22T22:48:14+08:00">2023-11-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-02-06 18:04:41" itemprop="dateModified" datetime="2024-02-06T18:04:41+08:00">2024-02-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">论文笔记</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>14 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>原文：《Swin transformer with multiscale 3D atrous convolution for
hyperspectral image classification》</p>
<h2 id="摘要">摘要</h2>
<p>高光谱图像（HSI）分类因其多样化的实际应用而引起了研究人员的极大兴趣。卷积神经网络
(CNN) 已广泛用于 HSI 分类。然而，基于 CNN
的方法的有效性受到卷积核的固定大小和结构以及它们无法捕获全局特征的限制。此外，这些网络不足以对数据的顺序特征进行建模。最近，出现了一种有前途的方法，即基于窗口的多头自注意力，以解决
CNN
的局限性并结合高效的序列建模功能。本文介绍了一种新颖的方法，即带有轻量级
Swin Transformer 的多尺度 3D 空洞卷积
(MACLST)，该方法有效地结合了两个网络的优势，以捕获 HSI
分类中不同尺度的局部和全局特征。MACLST 旨在处理 HSI
立方体作为输入，并采用基于多尺度 3D
空洞卷积的光谱空间特征提取模块。该模块涉及具有不同空洞率的 3D
层的并行分支，从而能够在多个尺度和分辨率下提取特征。提取的光谱空间特征被融合并作为线性嵌入传递到轻量级
Swin Transformer 模块。该模块捕获远程依赖关系并学习 HSI
的有效特征表示。为了降低计算复杂度，Swin Transformer
模块经过简化，仅包含两级，提供了原始 Swin Transformer
的更高效版本。所提出的 MACLST 模型在五个广泛使用的基准 HSI
数据集上进行了广泛评估，实验结果验证了其优于最先进的方法，在Indian
Pines，Pavia University，Salinas Valley，Houston University
2013和Houston University
2018数据集上的总体准确率分别为99.00%，99.59%，99.95%，98.71%和94.98%。</p>
<h2 id="本文思路">本文思路</h2>
<ol type="1">
<li>之前的方法主要侧重于表示光谱信息，而忽略了多尺度上下文信息在捕获 HSI
光谱空间特征中的潜在利用。可以观察到，同一类型的物体的光谱特性可能存在差异，而不同类型的物体则可能表现出相似的光谱特性。因此，仅依靠光谱特征不足以进行准确的
HSI
分类。空间特征捕获有关像素的空间排列和上下文的信息，在分类过程中也发挥着至关重要的作用。通过同时考虑光谱和空间特征，HSI分类模型可以更好地区分不同类别，提高分类结果。</li>
<li>虽然基于 CNN
的技术已成功提取空间和光谱特征，但它们也有一定的局限性。这些限制之一是它们难以捕获顺序属性，特别是中长期光谱依赖性。此外，它们提取短程特征的有效性受到固定大小的感受野的阻碍，这可能无法充分捕获数据中的细粒度细节和局部变化。</li>
</ol>
<p>文献表明，CNN
擅长提取局部光谱空间信息，但由于其固定的感受野大小，难以捕获远程光谱空间特征。另一方面，Transformer
已经显示出一种非凡的能力，可以理解远程特征之间的相互关系。集成 CNN 和
Transformer 用于 HSI 分类的有效性在很大程度上尚未得到探索。因此，在 HSI
分类的背景下结合这两种架构，通过结合短程和长程依赖关系，有可能增强光谱空间特征学习。</p>
<h2 id="本文贡献">本文贡献</h2>
<p>本文提出了一种新颖的方法，称为带有轻量级 Swin Transformer 的多尺度 3D
空洞卷积 (MACLST) ，该方法结合了两种最先进的技术：多尺度 3D 空洞卷积
(MAC) 和轻量级 Swin (LSwin) Transformer。目的是有效地学习 HSI
的判别性光谱空间信息。在第一个模块中，MACLST
采用具有不同空洞率的空洞卷积层的三个并行分支；
这种并行设计使模型能够学习多个尺度的特征并有效地提取丰富的光谱和空间信息。多尺度3D空洞卷积可以有效地计算密集特征图，在不显著增加参数和计算成本的情况下，使网络具有更大的感受野。这对于处理诸如
HSI 之类的高维数据尤其有利。在第二个模块中，LSwin Transformer
旨在学习特征序列之间的关系，使模型能够提取 HSI 的局部和全局特征。LSwin
Transformer
采用基于窗口的多头自注意力（W-MSA）机制，有效权衡图像中不同区域的重要性并捕获远程依赖关系。LSwin
Transformer
的分层性质通过允许跨窗口连接带来更高的效率，并表现出相对于图像大小的线性计算复杂性，确保高效处理。
特别是，与原来的四级 Swin Transformer 相比，所提出的 LSwinTransformer
模块被简化为仅包括两级。Stage
数量的减少可以使模型尺寸更紧凑，训练和推理时间更快，并在不影响性能的情况下减少内存和计算需求。通过集成
MAC 和 LSwin Transformer 的优势，所提出的 MACLST
模型在提取局部和全局特征方面都表现出色，从而有可能提高HSI分类的性能。本研究的主要贡献可以概括如下：</p>
<ol type="1">
<li>本研究引入了一种新的 HSI 分类方法，它集成了两种前言技术：多尺度 3D
空洞卷积和轻量级 Swin Transformer。</li>
<li>MAC 模块利用并行 3D
空洞卷积，在光谱和空间维度上应用不同的空洞率，通过显着扩大网络的感受野而不影响重要的分辨率信息，在捕获包含高判别能力的鲁棒光谱和空间特征方面发挥着关键作用
。</li>
<li>LSwin Transformer
模块在不影响精度的情况下显著降低了计算复杂度。它学习特征序列之间的关系，使网络能够对远程依赖性进行建模并提取强大的局部和全局特征表示。</li>
<li>通过在五个基准数据集上进行的实验，证明了所提出的 MACLST 模型相对于
HSI 分类中最先进的 (SOTA) 方法的有效性和优越性。</li>
</ol>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/article/202311222df0ba87.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://blog.tongorz.me/article/202310295eafc669.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/81060761_p0.jpg">
      <meta itemprop="name" content="Ahtong">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AhTong's blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | AhTong's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/article/202310295eafc669.html" class="post-title-link" itemprop="url">Swin Transformer：使用移位窗口的分层视觉Transformer</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-10-30 01:02:48" itemprop="dateCreated datePublished" datetime="2023-10-30T01:02:48+08:00">2023-10-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-02-06 17:53:06" itemprop="dateModified" datetime="2024-02-06T17:53:06+08:00">2024-02-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">论文笔记</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>12 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>原文：《Swin transformer with multiscale 3D atrous convolution for
hyperspectral image classification》</p>
<h2 id="摘要">摘要</h2>
<p>本文提出了一种新的视觉 Transformer，称为 Swin
Transformer，它能够作为计算机视觉的通用骨干。将 Transformer
从语言适应到视觉的挑战源于两个领域之间的差异，例如视觉实体的规模差异较大，以及图像中的像素与文本中的单词相比的高分辨率。为了解决这些差异，我们提出了一个分层
Transformer，其表示是使用移位窗口计算的。移位窗口方案通过将自注意力计算限制在非重叠的本地窗口，同时还允许跨窗口连接，带来了更高的效率。这种层次结构具有在各种尺度上建模的灵活性，并且具有相对于图像大小的线性计算复杂性。Swin
Transformer
的这些品质使其能够兼容广泛的视觉任务，包括图像分类（ImageNet-1K 上的
87.3 top-1 准确度）和物体检测等密集预测任务（COCO testdev 上的 58.7 box
AP 和 51.1 mask AP） 和语义分割（ADE20K val 上为 53.5 mIoU）。其性能在
COCO 上大幅超越了之前的最先进水平，为 +2.7 box AP 和 +2.6 mask AP，在
ADE20K 上为 +3.2 mIoU，展示了基于 Transformer
的模型作为视觉骨干的潜力。分层设计和移位窗口方法也被证明对全 MLP
架构有益。代码和模型可在https://github.com/microsoft/Swin-Transformer上公开获取。</p>
<h2 id="主要思路">主要思路</h2>
<p>计算机视觉中的建模长期以来一直由卷积神经网络 (CNN) 主导。从 AlexNet
及其在 ImageNet 图像分类挑战上的革命性表现开始，CNN
架构通过更大的规模、更广泛的连接和更复杂的卷积形式发展变得越来越强大。随着
CNN
作为各种视觉任务的骨干网络，这些架构的进步带来了性能的改进，广泛提升了整个领域的水平。
另一方面，自然语言处理 (NLP)
中网络架构的演变采取了不同的路径，今天流行的架构是 Transformer
。Transformer
专为序列建模和转导任务而设计，因其利用注意力来对数据中的远程依赖性进行建模而闻名。它在语言领域的巨大成功促使研究人员研究它对计算机视觉的适应性，最近在某些任务上展示了有希望的结果，特别是图像分类和联合视觉语言建模。在本文中，我们寻求扩展
Transformer 的适用性，使其可以作为计算机视觉的通用骨干，就像它在 NLP
中的作用以及 CNN
在视觉中的作用一样。我们观察到，将其在语言领域的高性能转移到视觉领域的重大挑战可以通过两种模式之间的差异来解释。其中一个差异涉及规模。与作为语言
Transformer 中处理基本元素的单词 token
不同，视觉元素在规模上可能存在很大差异，这是在目标检测等任务中需要注意的问题。在现有的基于
Transformer 的模型中，token
都是固定规模的，这一特性不适合这些视觉应用。另一个区别是图像中像素的分辨率比文本段落中的单词要高得多。存在许多视觉任务，例如语义分割，需要在像素级进行密集预测，这对于高分辨率图像上的
Transformer
来说是很棘手的，因为其自注意力的计算复杂度与图像大小成二次方。为了克服这些问题，我们提出了一种通用的Transformer主干，称为
Swin
Transformer，它构建分层特征映射，并且具有与图像大小相关的线性计算复杂度。
<img src="/article/202310295eafc669/image-20230918211430063.png" class="" title="image-20230918211430063"> 如图1(a)所示，Swin Transformer
通过从小尺寸的补丁(用灰色表示)开始并逐渐合并更深 Transformer
层中的相邻补丁来构建分层表示。借助这些分层特征图，Swin Transformer
模型可以方便地利用先进技术进行密集预测，例如特征金字塔网络 (FPN) [42] 或
U-Net
[51]。线性计算复杂度是通过在分割图像的非重叠窗口（以红色框出)内局部计算自注意力来实现的。每个窗口中的
patch 数量是固定的，因此复杂度与图像大小成线性关系。这些优点使 Swin
Transformer 适合作为各种视觉任务的通用骨干网，与之前基于 Transformer
的架构 [20] 形成鲜明对比，后者生成单一分辨率的特征图并具有二次方复杂度。
<img src="/article/202310295eafc669/image-20230918214630175.png" class="" title="image-20230918214630175"> Swin Transformer
的一个关键设计元素是连续自注意力层之间窗口分区的移动，如图 2
所示。移动的窗口桥接了前一层的窗口，提供了它们之间的连接，从而显着增强了建模能力（参见表
4)。这种策略在现实世界的延迟方面也很有效：窗口内的所有查询补丁共享相同的
key
集，这有利于硬件中的内存访问。相比之下，早期基于滑动窗口的自注意力方法由于不同
query 像素的 key
集不同，因此在通用硬件上延迟较低。我们的实验表明，所提出的移位窗口方法的延迟比滑动窗口方法低得多，但建模能力相似（参见表
5 和表 6)。事实证明，移位窗口方法对于全 MLP 架构也是有益的。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/article/202310295eafc669.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://blog.tongorz.me/article/20230917a675b6a1.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/81060761_p0.jpg">
      <meta itemprop="name" content="Ahtong">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AhTong's blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | AhTong's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/article/20230917a675b6a1.html" class="post-title-link" itemprop="url">利用所有未标记数据促进半监督学习</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-09-17 15:09:46" itemprop="dateCreated datePublished" datetime="2023-09-17T15:09:46+08:00">2023-09-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-02-06 17:48:23" itemprop="dateModified" datetime="2024-02-06T17:48:23+08:00">2024-02-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">论文笔记</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>12 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>原文：《Boosting Semi-Supervised Learning by Exploiting All Unlabeled
Data》</p>
<h2 id="摘要">摘要</h2>
<p>半监督学习（SSL）由于其在减轻对大型标记数据集的依赖方面的巨大潜力而引起了极大的关注。最新的方法（例如
FixMatch）结合使用一致性正则化和伪标签来取得显着的成功。然而，这些方法都浪费了复杂的样本，因为所有伪标签都必须通过高阈值来选择以滤除噪声标签。因此，预测不明确的样本不会对训练阶段做出贡献。为了更好地利用所有未标记的样本，我们提出了两种新技术：熵意义损失（EML）和自适应负学习（ANL）。EML将非目标类的预测分布纳入优化目标，以避免与目标类竞争，从而生成更多高置信度的预测来选择伪标签。ANL
为所有未标记的数据引入了额外的负伪标签，以利用低置信度样本。它通过动态评估模型的
top-k
性能来自适应地分配该标签。EML和ANL没有引入任何额外的参数和超参数。我们将这些技术与
FixMatch 集成，并开发了一个简单但功能强大的框架，称为
FullMatch。对几种常见 SSL 基准（CIFAR10/100、SVHN、STL-10 和
ImageNet）的大量实验表明，FullMatch 大幅超过 FixMatch。与
FlexMatch（一种基于 FixMatch
的高级框架）集成，我们实现了最先进的性能。源代码位于
https://github.com/megvii-research/FullMatch。</p>
<h2 id="本文思路">本文思路</h2>
<p>基于 FixMatch
的方法有一个显着的缺点，即<strong>它们依赖于极高的阈值来生成准确的伪标签，这导致忽略大量具有模糊预测的未标记示例，</strong>尤其是在早期和中期训练阶段。一种直观的解决方案是为潜在示例分配伪标签（即最大置信度接近预定义阈值）。<strong>我们认为部分类别之间的竞争导致无法产生高置信度的预测，而在使用伪标签训练示例时，FixMatch（即交叉熵）的无监督损失仅关注目标类别。</strong>因此，我们提出了一种新的方案来增强目标类别的置信度，即熵意义损失（EML）。对于带有伪标签的示例，EML
对所有非目标类（即指定不存在特定标签的类）施加额外的监督，以使它们的预测接近均匀分布，从而防止与目标类发生任何类竞争。由于
EML
试图产生更多的低熵预测来选择更多带有伪标签的示例，而不是调整阈值，因此它也可以应用于任何动态阈值方法。
尽管如此，仍然不可能通过阈值策略生成伪标签来利用所有未标记的数据。这促使我们进一步考虑如何利用没有伪标签的低置信度未标记示例（即最大置信度远离预定义阈值）。直观上，预测可能会在
top 类别之间混淆，但可以确信输入不属于排在这些类别之后的类别。
<img src="/article/20230917a675b6a1/image-20230805121006693.png" class="" title="image-20230805121006693"> 图2展示了 FixMatch 的推理结果。真实类别是“猫”，FixMatch
被几个 top
类别（例如“狗”、“青蛙”）混淆并做出低置信度预测，但它显示出对某些低等级类别（例如“飞机”、“马”)不是真实类别的高度置信度，因此我们可以安全地为这些类别分配负伪标签。基于这一见解，我们提出了一种名为自适应负学习（ANL）的新方法。具体来说，ANL
首先根据预测一致性自适应计算 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.179ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 521 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g></svg></mjx-container></span>
，使得 top-<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.179ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 521 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g></svg></mjx-container></span>
的准确率接近1，然后将排名在 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.179ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 521 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g></svg></mjx-container></span>
之后的类视为负伪标签。此外，如果样本被选为伪标签，ANL
将缩小非目标类的范围（即EML只需要约束除目标类之外的 top-<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.179ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 521 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g></svg></mjx-container></span> 类)。请注意，ANL
是一种与阈值无关的方案，因此可以应用于所有未标记的数据。</p>
<h2 id="本文贡献">本文贡献</h2>
<ol type="1">
<li>在使用伪标签训练样本时，我们引入了一种额外的监督，即熵意义损失（EML），它强制非目标类的均匀分布，以避免它们与目标类竞争，从而产生更多的高置信度预测。</li>
<li>我们提出了自适应负学习（ANL），这是一种动态负伪标签分配方案，它以非常有限的额外计算开销的为所有未标记数据（包括低置信度样本）设置负伪标签。</li>
<li>我们设计了一个简单而有效的框架，名为 FullMatch，通过简单地将
FixMatch 与所提出的 EML 和 ANL
集成，它利用所有未标记的数据，从而在五个基准上取得显着的收益。</li>
</ol>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/article/20230917a675b6a1.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://blog.tongorz.me/article/202308032166b3a9.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/81060761_p0.jpg">
      <meta itemprop="name" content="Ahtong">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AhTong's blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | AhTong's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/article/202308032166b3a9.html" class="post-title-link" itemprop="url">用于高光谱图像分类的光谱空间特征标记化Transformer</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-08-03 18:59:43" itemprop="dateCreated datePublished" datetime="2023-08-03T18:59:43+08:00">2023-08-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-02-06 17:37:18" itemprop="dateModified" datetime="2024-02-06T17:37:18+08:00">2024-02-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">论文笔记</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>11 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>原文：《Spectral-Spatial Feature Tokenization Transformer for
Hyperspectral Image Classification》</p>
<h2 id="摘要">摘要</h2>
<p>在高光谱图像 (HSI) 分类中，每个像素样本都分配给一个土地覆盖类别。
近年来，基于卷积神经网络（CNN）的 HSI
分类方法由于其卓越的特征表示能力而大大提高了性能。
然而，这些方法获取深层语义特征的能力有限，并且随着层数的增加，计算成本显着上升。
Transformer框架可以很好地表示高级语义特征。
在本文中，提出了一种光谱空间特征标记化Transformer（SSFTT）方法来捕获光谱空间特征和高级语义特征。
首先，构建光谱空间特征提取模块来提取低级特征。
该模块由3维卷积层和2维卷积层组成，用于提取浅层光谱和空间特征。
其次，引入高斯加权特征Transformer进行特征转换。
第三，将变换后的特征输入到Transformer编码器模块中进行特征表示和学习。
最后，使用线性层来识别第一个可学习的标记以获得样本标签。
使用三个标准数据集，实验分析证实，计算时间少于其他深度学习方法，并且分类性能优于当前几种最先进的方法。
为了重现性，这项工作的代码可以在 https://github.com/zgr6010/HSI_SSFTT
上找到。</p>
<h2 id="本文思路">本文思路</h2>
<p>大多数方法都是基于 CNN
主干及其变体。虽然这些方法有效提高了HSI分类性能，但由于训练样本有限和网络层数增加造成的分类性能下降难以克服。
它们还具有过多的功能冗余。
最近，一种名为视觉Transformer（ViT）的新模型在图像处理领域表现良好。已经做了一些工作来将Transformer模型应用于HSI分类。然而，大部分方法都是基于光谱信息处理的改进的Transformer方法。尽管Transformer在捕获光谱特征方面表现突出，但它在捕获局部语义特征方面失去了能力，并且没有充分利用图像空间信息。
原始的Transformer[60]是基于自注意力（SA）机制的应用于自然语言处理（NLP）的模型。
模型的输入是一系列标记。 多头注意力用于绘制输入标记序列中的全局相关性。
因此，为了利用Transformer获取局部空间语义信息的能力并对相邻序列之间的关系进行建模，提出了一种用于
HSI 分类的谱空间特征标记化Transformer（SSFTT）模型。
首先，在该模型中，使用 3-D 卷积层和 2-D 卷积层来提取浅层光谱空间特征。
这有效地减少了层数增加带来的特征冗余和不准确。
其次，展平的特征由高斯加权标记器进行标记。然后，生成的令牌用作 TE
模块的输入。 最后，采用基于 softmax
的线性分类器来确定每个像素的标签。</p>
<h2 id="本文贡献">本文贡献</h2>
<ol type="1">
<li>我们的 SSTFF 网络中提出了一种简单高效的分层 CNN
模块，用于提取浅层空间光谱特征。 它仅由1个 3D 卷积层和1个 2D
卷积层组成。 然后，该模块与 Transformer
结构相结合，开发出一种新的轻量级网络来替代单个 CNN
结构，以降低计算成本。</li>
<li>提出了高斯分布加权标记化模块，将浅层空间谱特征转换为标记化语义特征。
其作用是使token所表达的深层语义特征更加符合样本的分布特征，从而使样本更加可分。</li>
<li>CNN网络与Transformer结构从浅到深的系统结合，可以充分利用HSI中的光谱空间信息，简洁高效地表达HSI的低中深语义特征，从而显着提高分类精度。</li>
</ol>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/article/202308032166b3a9.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Ahtong</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">114k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">6:20</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div><script color="0,0,0" opacity="0.5" zIndex="-1" count="99" src="https://cdn.jsdelivr.net/npm/canvas-nest.js@1/dist/canvas-nest.js"></script>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  
  <script async src="https://busuanzi.icodeq.com/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



  <script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/moment.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/moment-precise-range-plugin@1.3.0/moment-precise-range.min.js"></script>
  <script>
    function timer() {
      var ages = moment.preciseDiff(moment(),moment(20240130100000,"YYYYMMDDhmmss"));
      ages = ages.replace(/years?/, "年");
      ages = ages.replace(/months?/, "月");
      ages = ages.replace(/days?/, "天");
      ages = ages.replace(/hours?/, "小时");
      ages = ages.replace(/minutes?/, "分");
      ages = ages.replace(/seconds?/, "秒");
      ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
      div.innerHTML = `本站已稳定运行 ${ages}`;
    }
    var div = document.createElement("div");
    var copyright = document.querySelector(".copyright");
    document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
    timer();
    setInterval("timer()",1000)
  </script>

  
<script class="next-config" data-name="giscus" type="application/json">{"enable":true,"repo":"7ongOrz/comments","repo_id":"R_kgDOLMZT5A","category":"Announcements","category_id":"DIC_kwDOLMZT5M4CdAO0","mapping":"url","reactions_enabled":1,"emit_metadata":1,"theme":"preferred_color_scheme","lang":"zh-CN","crossorigin":"anonymous","input_position":"top","loading":"lazy"}</script>

<script>
document.addEventListener('page:loaded', () => {
  if (!CONFIG.page.comments) return;

  NexT.utils.loadComments('.giscus-container')
    .then(() => NexT.utils.getScript('https://giscus.app/client.js', {
      attributes: {
        async                   : true,
        crossOrigin             : 'anonymous',
        'data-repo'             : CONFIG.giscus.repo,
        'data-repo-id'          : CONFIG.giscus.repo_id,
        'data-category'         : CONFIG.giscus.category,
        'data-category-id'      : CONFIG.giscus.category_id,
        'data-mapping'          : CONFIG.giscus.mapping,
        'data-reactions-enabled': CONFIG.giscus.reactions_enabled,
        'data-emit-metadata'    : CONFIG.giscus.emit_metadata,
        'data-theme'            : CONFIG.giscus.theme,
        'data-lang'             : CONFIG.giscus.lang,
        'data-input-position'   : CONFIG.giscus.input_position,
        'data-loading'          : CONFIG.giscus.loading
      },
      parentNode: document.querySelector('.giscus-container')
    }));
});
</script>

</body>
</html>
